[
 {
  "title": "Advanced SQL in Starburst",
  "url": "/videos/2020-07-29-advanced-presto-sql.html",
  "content": "Advanced SQL in StarburstHosts: David Phillips, Manfred MoserVideo date: 9 July 2020Running time: 2h14m    This training session is geared towards helping users understand how to run morecomplex and comprehensive SQL queries with Starburst. Delivered by DavidPhillips, this session covers the following topics:  Using JSON and other complex data types  Advanced aggregation techniques  Window functions  Array and map functions  Lambda expressions  Many other SQL functions and featuresDetailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.  Welcome - 0:00  General SQL Features - 9:45          Format function - 9:57      Case expressions - 12:48      Searched case expression - 13:34      IF expression - 14:24      TRY expression - 15:26      Lambda expression overview        Using JSON - 18:36          JSON data type - 19:13      Extraction using JSONPath - 23:11      Casting from JSON - 26:01      Parting casting from JSON - 27:07      Formatting as JSON - 29:40        Advanced Aggregation Techniques - 32:50          Counting distinct items - 33:06      Approximate percentiles - 36:02      Associated max value - 37:59      Associated max value using a row type - 38:39      Pivoting with conditional counting - 40:58      Pivoting with filtering - 42:08      Pivoting averages - 42:57      Aggregating a complex expression - 43:55      Aggregating into an array - 45:36      Aggregating into a lambda - 46:28      Order-insensitive checksums - 50:07      ROLLUP with single - 52:32      ROLLUP with multiple - 53:24      CUBE - 55:52      GROUPING SETS - 59:28        Array and Map Functions - 1:07:47          Creating arrays - 1:08:40      Accessing array elements - 1:09:07      Sorting arrays - 1:11:25      Matching elements - 1:13:26      Filtering elements - 1:14:57      Transforming elements - 1:16:25      Converting arrays to strings - 1:17:44      Computing array product - 1:19:45      Unnesting an array - 1:22:16      Unnesting an array with ordinality - 1:23:06      Creating maps - 1:23:44      Accessing map elements - 1:25:20      Unnesting a map - 1:26:16        Window Functions - 1:28:00          Window function overview - 1:28:58      Row numbering - 1:30:04      Row numbering order - 1:30:49      Row numbering with limit - 1:31:39      Rank - 1:32:30      Rank with ties - 1:33:11      Dense rank with ties - 1:33:54      Ranking without ordering - 1:34:20      Row numbering without ordering - 1:34:40      Assigning rows to buckets - 1:36:07      Percentage ranking - 1:37:12      Partitioning - 1:37:53      Partitioning on the same value - 1:39:26      Accessing leading and trailing rows - 1:40:12      Accessing leading and trailing rows with nulls - 1:42:56      Accessing leading and trailing rows without nulls - 1:43:56      Window frames - 1:45:13      Accessing the first value - 1:47:05      Accessing the last value - 1:47:35      Accessing the Nth value - 1:47:49      Window frame ROWS vs RANGE - 1:48:15      Rolling and total sum - 1:50:39      Partition sum - 1:52:00      "
 },
 {
  "title": "Understanding and tuning Starburst query processing",
  "url": "/videos/2020-08-12-query-performance.html",
  "content": "Understanding and tuning Starburst query processingHosts: Martin Traverso, Manfred MoserVideo date: 12 August 2020Running time: 2h09m    This training session is geared towards helping users understand howStarburst executes queries. That knowledge can help you improve queryperformance. For instance, the explain plan is a powerful tool. We explore howto access the explain plan and how to read it. We look at the work thecost-based optimizer performs and how you can potentially help run yourqueries even faster. Delivered by Martin Traverso, this session covers thefollowing topics:  Explain the EXPLAIN  Learn how queries are analyzed and executed  Understand what the optimizer does, including some of its limitations  Showcase the cost-based optimizerDetailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can skip tothat timestamp in the video player above.  Welcome - 0:00  Query lifecycle - 8:11          Parsing - 9:44      Analysis - 11:29      Planning - 15:25      Optimization - 16:44      Scheduling and execution - 18:50        Explain the EXPLAIN - 20:18          EXPLAIN command - 20:55      EXPLAIN vs EXPLAIN ANALYZE - 23:07      Fragment Structure - 24:06      Distribution - 26:55      Row layout - 29:37      Estimates - 31:54      Performance stats - 33:49      Exchanges - 39:33        Optimization - 41:10          Constant folding - 43:23      Predicate pushdown - 51:07      Predicate pushdown into connectors - 55:43      Predicate pushdown into the Hive connector - 1:08:24      Hive partition pruning - 1:10:18      Hive bucket pruning - 1:17:12      Row group skipping for ORC and Parquet - 1:19:17      Limit pushdown - 1:22:31      Partial limit pushdown - 1:26:11      Aggregation pushdown - 1:28:26      Skew - 1:33:41        Cost-based Optimizations - 1:39:01          Partitioned join - 1:41:41      Broadcast join - 1:44:01      Join type selection - Partitioned - 1:46:02      Join type selection - Broadcast - 1:47:07      Disabling cost-based optimizations - 1:48:10      Join reordering - 1:48:44      Table statistics - 1:50:21      Computing statistics - 1:51:19        Resources - 1:52:09"
 },
 {
  "title": "Securing Starburst Enterprise",
  "url": "/videos/2020-08-26-securely-deploy-presto.html",
  "content": "Securing Starburst EnterpriseHosts: Dain Sundstrom, Manfred MoserVideo date: 26 August 2020Running time: 2h07m    This training session is geared towards helping Starburst Enterprise platform (SEP) userssecurely deploy SEP at scale. We cover how to secureSEP as well as access to your underlying data. Delivered by DainSundstrom, this session covers the following topics:  Authentication, including password &amp;amp; LDAP Authentication  Authorization to access your data sources  Encryption including client-to-coordinator communication  Secure communication in the cluster  Secrets usage for configuration files including catalogsDetailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.  Welcome - 0:00  Tips and Notes - 5:06          Process for securing SEP - 7:34      What to secure - 11:02      Verify HTTP with the Web UI - 13:23      Verify HTTP with the CLI - 14:48                  CLI failures - 15:14                    Client to Server Encryption - 15:44                  Approaches for HTTPS - 15:58          HTTPS proxy or load balancer - 17:33          Add the SSL/TLS certificate to the coordinator - 20:28                          Inspect the PEM file - 22:40              Verify the PEM file certificate - 23:45              Verify the PEM private key - 26:08              Verify the JKS file - 26:38              Configure SEP - 27:59                                Verify HTTPS with the Web UI - 28:51          Verify HTTPS with the CLI - 29:36                          CLI failures - 29:46              CLI –insecure - 33:23                                          Authentication - 34:57                  Password file authentication - 36:08          LDAP Authentication - 41:19          Kerberos Authentication - 50:24          Client certificate authentication - 53:53          JSON Web Token authentication - 55:03          Multiple authenticators - 56:01          User mapping - 58:14                    Authorization - 1:00:08                  File-based system access control - 1:02:54                    Client to server summary - 1:07:23      Internal security and connector security - 1:18:14                  Securing the cluster itself - 1:18:30          Shared secret - 1:20:29                    Internal HTTPS - 1:23:58      Secrets Management - 1:27:53      Management Endpoints - 1:30:23      Hive Catalog Security - 1:33:29                  Hive Catalog Authorization - 1:34:45          Hive Metastore Authentication - 1:38:08          HDFS Authentication - 1:42:24          Hive Kerberos Debugging - 1:43:31          S3 Authentication - 1:45:53          GCP Authentication - 1:49:31                    "
 },
 {
  "title": "Starburst cluster sizing, and performance tuning",
  "url": "/videos/2020-09-09-cluster-sizing-and-performance-video.html",
  "content": "Starburst cluster sizing, and performance tuningHosts: Dain Sundstrom, Manfred MoserVideo date: 9 September 2020Running time: 2h15m    This training session is geared towards helping users tune and size theirdeployment for optimal performance. Delivered by Dain Sundstrom, this sessioncovers the following topics:  Cluster configuration and node sizing  Memory configuration and management  Improving task concurrency and worker scheduling  Tuning your JVM configuration  Investigating queries for join order and other criteria  Tuning the cost-based optimizerDetailed topics with timestampsClicking the timestamp links below will take you to YouTube, or you can scrub tothat timestamp in the video player above.  Welcome - 0:00  General Strategy - 5:45  Baseline Advice - 11:53  Cluster Sizing / CPU and Memory - 14:57  Machine Sizing - 34:46          Memory - 34:58      Memory Allocations - 39:55      Shared Join Hash - 46:59      Distributed Join - 49:11      Skew - 50:58      Use bigger machines - 55:03      Machine Types - 58:40        Additional Thoughts - 1:03:20          Hash join vs (sort) merge join - 1:03:25      Spilling - 1:04:43      Small Clusters - 1:07:54        Tuning the Workload - 1:23:45          Query Plan - 1:24:52      Precomputing - 1:30:00      Connectors - 1:34:53        Hive Data Organization - 1:39:27          Organize the data for the Hive connector - 1:39:41      Hive Partitioning - 1:42:13      Hive bucketing - 1:43:59      Orc and Parquet - 1:46:26      File Size - 1:51:02      Bad Parquet Files - 1:53:13      Rewrite table with ORC writer - 1:54:29        Making Queries Faster - 1:55:37          What to look for in a query - 1:57:09      More hardware - 2:01:18      Under-utilization - 2:02:32      Hive Caching - 2:05:29        Sharing Resources / Resource Groups - 2:08:34"
 },
 {
  "title": "404",
  "url": "/404.html",
  "content": "                                  Oops ... 404        Don&#39;t panic. We brought a towel.        Try going home.        Or maybe you are looking for some specific documentation and resources:                  Starburst Enterprise          Starburst personas overview          Platform administrator          Data engineer          Data consumer                            &amp;nbsp;                                          Confused where you landed?          Go          straight to the Starburst Enterprise reference documentation!          A lot of new useful documentation is available here, but fear not,          all the comprehensive reference documentation for Starburst Enterprise is ready and constantly improving as always.                                          "
 },
 {
  "title": "Add a MySQL data source",
  "url": "/starburst-galaxy/data-sources/mysql/add-mysql.html",
  "content": "Add a MySQL data sourceFrom the Data sources page, click Add a new data source or + New,select MySQL to begin adding your datasource.Basic informationThe name and description that you choose for your data source allows you to tellit apart from other data sources at a glance.  Enter a unique Catalog name for your data source. The naming requirementssupport lowercase letters, numbers, and underscores.  Add a Description to share more details about the data source thanjust the name.  Click Next to access Connection details.Provide connection detailsEnter the information needed to connect with your MySQL database.  Add your Host and Port number.  Enter the Connection URL.  Add your User and Password secret name.  Select Next to continue to Advanced settings.Advanced settingsAny additional settings are optional, to finish creating your data source:  Click Test connection to data source.  After the connection is established, select Finish.Return to the Data sources page at any time to edit your existing datasources or add new settings to them.Optional settingsThe optional advanced setting include:  Types handling  Case insensitive name matching  Metadata caching  Table statisticsAdd, edit, or remove these settings at any time."
 },
 {
  "title": "Add a PostgreSQL data source",
  "url": "/starburst-galaxy/data-sources/postgresql/add-postgresql.html",
  "content": "Add a PostgreSQL data sourceFrom the Data sources page, click Add a new data source or + New,select PostgreSQL to begin adding your datasource.Basic informationThe name and description that you choose for your data source allows you to tellit apart from other data sources at a glance.  Enter a unique Catalog name for your data source. The naming requirementssupport lowercase letters, numbers, and underscores.  Add a Description to share more details about the data source thanjust the name.  Click Next to access Connection details.Connection detailsEnter the information needed to connect with your MySQL database.  Add your Host and Port number.  Enter the Database and Connection URL, the Connection URL isoptional.  Add your User and Password secret name.  Select Next to continue to Advanced settings.Advanced settingsAny additional settings are optional, to finish creating your data source:  Click Test connection to data source.  After the connection is established, select Finish.Return to the Data sources page at any time to edit your existing datasources or add new settings to them.Optional settingsThe optional advanced setting include:  Enable user impersonation  Include system tables  Types handling  Case insensitive name matching  Metadata caching  Table statisticsAdd, edit, or remove these settings at any time."
 },
 {
  "title": "Add an Amazon S3 data source",
  "url": "/starburst-galaxy/data-sources/s3/add-s3.html",
  "content": "Add an Amazon S3 data sourceFrom the Data sources page, click Add a new data source or + New,select Amazon S3 to begin adding your datasource.Basic information  Enter a unique Name for your data source. The naming requirements supportlowercase letters, numbers, and underscores.  Add a Description to share more details about the data source than justthe name.  Click Next to access Connection details.Connection detailsShare your metastore connection details. These details giveStarburst Galaxy access to metadata and mapping information about theobjects stored in S3.Use AWS Glue metastores  that you already saved inthe Starburst Galaxy platform (SGP). Save new metastores that you’ve already createdin AWS to SGP.Use a saved AWS Glue metastoreAdd an existing AWS Glue metastorethat you already saved in SGP to your connection details:  In the Provide metastore connection details section, select AWS Glue.  Select a saved AWS Glue metastore from the dropdown menu.  Select Secret key or Assumed IAM role.          Secret key: Enter the Access key ID and yourSecret access key name. The secret key ID isthe name of your secret key in your secret manager.      Assumed IAM role: Enter the IAM role ARN.        Click Next to access the Security section.Add a new AWS Glue metastore to SGPDefine and connect Starburst Galaxy to a AWS Glue metastore:  In the Provide metastore connection details section, select AWS Glue.  Check the Add a new AWS Glue metastore box.          Enter a Name for your AWS Glue metastore.      Select the AWS Glue region from the drop down menu.      Enter the Access key ID and your Secret access key name. The secretkey ID is not your password, it is the name of your secret key in yoursecret manager.        Select Secret key or Assumed IAM role.          Secret key: Enter the Access key ID and your Secret access key name. The secret key ID is the name of your secret key in your secret manager.      Assumed IAM role: Enter the IAM role ARN.        Click Next to access the Security section.SecuritySelect your data source authentication and authorization type:  Default authorization  SQL standard authorization  Read only authorizationDefault authorizationRequires fewer authorization checks which allows most operations.  Select the Default authorization type. Edit the Column options,Table options, or both.          Column options: Allow column rename, drop, or add.      Table options: Allow table rename, drop, or add.        Click Next to finish connecting your data source.You can edit or delete an existing data source at any time.SQL standard authorizationRequires SQL standard privileges to perform operations. Starburst Galaxyenforces the authorization checks for queries based on the privileges defined inmetastore.  Select SQL Standard authorization type.  Click Test connection to data source to establish the connection with S3.  Click Finish.You can edit or delete an existing data source at any time.Read only authorizationEnabling read only data source security allows operations that read data ormetadata, such as SELECT. It disables operations that write data ormetadata, such as CREATE, INSERT, or DELETE.  Select Read only authorization type.  Click Next to finish connecting your data source.You can edit or delete an existing data source at any time."
 },

 {
  "title": "Trying Starburst Enterprise on any Linux",
  "url": "/starburst-enterprise/try/any-linux.html",
  "content": "Trying Starburst Enterprise on any LinuxYou can install Starburst Enterprise platform (SEP) on any 64-bit Linux distribution withthe following steps:  Obtain the most recent SEP tar.gz archive  Unpack the archive as root  Add configuration files  Start the SEP server  Obtain the Presto CLI client and run testsPrerequisitesStarburst Enterprise requires a Linux distribution that:  Is no more than a few years old  Runs on 64-bit Intel hardware  Has Python 2.7 or later (only needed to run the launcher utility)  Has Java 11.0.7 or a later Java 11 LTS release from OpenJDK, Oracle, or AzulJava distributions. (Newer Java releases may work but are not tested.)(SEP also installs for evaluation purposes only on a macOSrelease with the same prerequisites.)Download an SEP archiveTo gain access to SEP archives, visit the Starburstwebsite and click either theGet Started or Start Free buttons.This opens a dialog that prompts for your name, email address, and location.Fill out the form using a valid email address, then click Free Download.A few moments later, you receive email from Starburst with a link to thedownloads page.   Note:  You can optionally reply to the email fromStarburst to request a trial license that unlocks the performance and securityenhancements of Starburst Enterprise. However, to get a server up and runningquickly, you can postpone using a license for now.The Downloads page is organized into Long-Term and Short-Term Support sections.Choose an LTS edition.Download the tar.gz file from the All Platforms LTS link.Download the Presto CLI JAR fileWhile you’re on the download page described above, download thePresto CLI client JAR file that matches the version of the serveryou downloaded. That is, download the file whose name ends withexecutable.jar. We will take advantage of this file later.Unpack the archiveThe contents of the tar.gz archive are by default owned by root. Select anappropriate location and unpack the files as usual. (This page uses /usr/localas the location.) For example:cd /usr/localsudo tar xvzf ~/Downloads/presto-server-nnn.tar.gzwhere nnn is the version number you downloaded.This creates a directory named the same as the basename of the tar.gz file inthe current directory, such as starburst-server-nnn. We refer to this as thesep-root directory.Your site may have standards for running servers that includes not running asroot. If you do not want the SEP server to run as root, create auser named sep and change ownership to sep of all files and directories inthe new sep-root directory. Then log in as sep to run the upcoming commands.Inspect the new directory to find that it contains three directories:binlibpluginAdd configuration filesEven the simplest Starburst Enterprise server must have a minimum set ofconfiguration files before it can start. Create a directory named etc parallelto bin and lib. Populate etc with the following configuration files, usingcontents suggested on Deploying Presto.  node.properties  Follow the sample in Node properties. For the node.properties  line, the suggested value production has no special meaning; use any  value, such as test.  jvm.config  Follow the sample in JVM config. Use the suggested text verbatim, except reduce  the -XmX value as appropriate for your test environment.  config.properties  Follow the sample in  Config properties. Use the third suggestion for a combined coordinator and  worker machine.  catalog.properties  Follow the sample in  Catalog properties. Create a subdirectory catalog under etc. In there, create at least one file for each data source. To start with, create a file named tpch.properties  with the following s  contents is the single line:  connector.name=tpchAlternate configuration filesAnother way to get started quickly is to use the set of configuration filesprovided as examples for the O’Reilly book Presto: The DefinitiveGuide.To use these ready-to-use configuration files, download the samples from theirGitHub locationeither as a zip file or a git clone. Let’s say you place your clone or unzipdirectory in ~/bookfiles.From bookfiles, copy the entire etc directory from the single-installationsample folder to your sep-root directory. For example:cd ~/bookfiles/single-installationsudo rsync -av etc /usr/local/starburst-server-nnn/Start the serverOnce configuration files are in place, start the server. From the sep-rootdirectory, run the following command:sudo bin/launcher startFor a successful server start, the launcher script returns a process ID.For a sep-root directory in which you changed file and subdirectory ownership,you can drop the sudo or use sudo -U sep to run bin/launcher.Verify the serverTo verify that your locally-run server is operating as expected, invoke thePresto UI as described in Verify theserver.Run queriesTo run queries against your server, use the Presto CLI as describedin CLI."
 },
 {
  "title": "Get started",
  "url": "/starburst-galaxy/aws-create-role.html",
  "content": "Get startedUsing Starburst Galaxy platform (SGP) requires you to configure your AWSaccount access for running the clusters.Configure your AWS accountYou need to configure a cross-account role for your AWS account. This enablesthe SGP to deploy and manage Starburst clusters inyour account.Create a cross account roleCreate a cross-account role using a cloud formation template (CFT) in theAWS management console.  In your AWS management console, go to IAM services. You may be prompted to sign in to AWS.  Download the Cloud formation template from Starburst Galaxy.  Upload the Cloud formation template in your AWS console.  Copy the External ID from Starburst Galaxy and paste it in yourAWS console.  Run the cloud formation template.  IMPORTANT: Copy the AWS Account ID where you created the role. Youneed it for the next steps.Enter the cross-account role informationAdd the cross-account information to Starburst Galaxy.  Navigate back to Starburst Galaxy  In the AWS Account ID field, enter the AWS Account ID from the roleyou created.  Select the default AWS region for your AWS resources.  Click Finish.You can edit your AWS region from Settings or on individual clusters asneeded."
 },
 {
  "title": "Configure and define catalogs",
  "url": "/starburst-enterprise/data-engineer/catalogs.html",
  "content": "Configure and define catalogsYou need to understand data sources and how they are connected toStarburst Enterprise platform (SEP), to take advantage of the query performance availableto your data consumers. The following content provides an overview of datasources and catalogs and how they work with connectors in SEP.DefinitionsBefore you begin, here are definitions and explanations the key concepts of fordata sources, catalogs, and connectors.Data sourcesA data source is a system where data is retrieved from. You can query systemssuch as distributed object storage, RBDMSs, NoSQL databases, document databases,and many others.In SEP, you must connect to a data source so you can query fromthat source. To query those sources, you need to create a catalog propertiesfile to define a catalog.CatalogsCatalogs define and name the configuration to connect to and query a datasource. They are a key concept inSEP.Without catalogs there is nothing to query.The catalog name is defined by the name of the catalog properties file, locatedin etc/catalog. You can have as many catalogs as you want, and there are norestrictions for naming outside of valid character types. For example, afilename of etc/catalog/mydatabase.properties results in the catalog namemydatabase.The catalog properties file content defines the access configuration to the datasource. Properties are key=value pairs.Each catalog defines one and only one connector using the requiredconnector.name property. You must select the correct connector for your datasource. For example, the PostgreSQL connector is defined by usingconnector.name=postgresql, and enables the catalog to access a PostgreSQLdatabase. Another example is the Hive connector defined byconnector.name=hive-hadoop2. It can enable a catalog to access Hadoop,Amazon S3 and many other object storage systems.   Note:  While a catalog can have only one connector, asingle connector may be used by multiple catalogs.Access a list of catalogs with theSHOW CATALOGS command.Configuration propertiesEach connector has a small set of required properties. While properties canvary, they minimally define the connection to the data source. Depending on yourconnector and data source, there are a number of different configurationproperties available.Optional properties enable further configuration of the catalog in areas such assecurity, performance, and query behavior. These connector-specific propertiesare defined in the documentation for each connector. For more information onconnector-specific configuration properties, start with the list of allconnectors.Catalog session propertiesYou can further customize the behavior of the connector in your catalog usingcatalog session properties. A session is defined by a specific user accessingSEP with a specific tool such as the CLI. Catalog sessionproperties can control resource usage, enable or disable features, and changequery processing.Most of the session properties are similarly named to their config propertiescounterparts in a catalog file, mostly differing by the use of underscores (_)in the name to be SQL compliant. Session properties override catalog propertiesin certain circumstances.You can view current session properties using theSHOW SESSION command. Thenimplement your session properties usingSET SESSION.ConnectorsA connector is specific to the data source it supports. It transforms theunderlying data into the SEP concepts of schemas, tables,columns, rows, and data types.Connectors provide the following between a data source and SEP:  Secure communications link  Translation of data types  Handling of variances in the SQL implementation, adaption to a provided API,or translation of data in raw filesSetup of connectorMost SEP connectors include their configurations and anythingelse you might need by default, and therefore no setup is required.If you’re using a connector that requires additional set up, such as theaddition of a proprietary JDBC driver, you find that documented with thespecific connector.Create a catalog properties fileYou can create catalog to access a data source with a few simple steps:  Create the catalog properties file in etc/catalog/, for exampleetc/catalog/mydatabase.properties  Specify the required connector with connector.name= in file, for exampleconnector.name=postgresql  Add any other properties required by the connector  Add any optional properties as desired  Copy the file onto the coordinator and all worker nodes  Restart the coordinator and all workers  Confirm the catalog is available with SHOW CATALOGS;  List the available schemas with SHOW SCHEMAS FROM mydatabase;  Start writing and running the desired queriesMany connectors use the similar properties in catalog properties files.Most JDBC-based connectors require these minimum properties for their catalogfiles:connector.name=[connectorname]connection-url=[connectorprotocol]//&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;;database=&amp;lt;database&amp;gt;connection-user=rootconnection-password=secretExamples of connector protocols include:# JDBC-based connectorconnection-url=jdbc:postgresql://example.net:5432/database# Endpoint-driven connectorhive.cos.service-config=etc/catalog/cos-service.properties"
 },
 {
  "title": "CLI",
  "url": "/data-consumer/clients/cli.html",
  "content": "CLIThe Presto command line interface (CLI) provides a terminal-based,interactive shell for running queries and inspecting catalog structures in anySEP cluster.Setup to use the CLIThe CLI is distributed as an executable JAR file that you download, rename, andplace in a directory in the PATH.RequirementsThe CLI requires a java command on the PATH from Java 8 or newer. It isusually easiest to use the same Java 11 required by SEP itself,described on Java runtimeenvironment.Download the CLITo gain access to SEP archives, visit the Starburstwebsite and click either the Get Started orStart Free buttons.This opens a dialog that prompts for your name, email address, and location.Fill out the form using a valid email address, then click Free Download.A few moments later, you receive email from Starburst with a link to thedownloads page. The Downloads page is organized into Long-Term and Short-TermSupport sections.Select and download the latest Presto CLI executable JAR fileavailable, which is usually in the STS section.You can also download the CLI binary straight from the link in the referencedocumentationRename and place the CLIYou downloaded an executable JAR file that is usable as-is. To make it practicalwith having to call Java, copy it to a directory in the PATH, rename it, andmake it executable. For example:cd /usr/local/bincp -p /home/&amp;lt;yourname&amp;gt;/Download/presto-cli-nnn-executable.jar .mv presto-cli-nnn-executable.jar prestochmod +x prestowhere nnn is the version of the file you selected.First stepsNow you are ready to verify that the CLI runs. Check if you get a similar outputto the following listing:$ presto --versionPresto CLI 350The CLI works. Now gather the connection information for your cluster, and connect to it. You can learn more about usingthe CLI from the following tutorial or the Command line interfacesection in the SEP referencedocumentation.CLI tutorialThe following sections provide a brief introduction to using the CLI.Interactive CLIAt the shell prompt, enter presto with no arguments. By default, this connectsto the running SEP server at the default address and port,localhost:8080.prestoIf your SEP server uses a different port or is running elsewhereon your network, specify the server’s URL with --server. For example:presto --server=presto.example.com:8082This opens a Presto CLI shell, with presto&amp;gt; prompt. All commandsentered here must be terminated with a semicolon.Exit presto’s interactive mode with quit; or exit; or Ctrl+D:presto&amp;gt; quit;Usage helpTo see the available commands in interactive mode:presto&amp;gt; help;This returns:Supported commands:QUITEXPLAIN [ ( option [, ...] ) ] &amp;lt;query&amp;gt;    options: FORMAT { TEXT | GRAPHVIZ | JSON }             TYPE { LOGICAL | DISTRIBUTED | VALIDATE | IO }DESCRIBE &amp;lt;table&amp;gt;SHOW COLUMNS FROM &amp;lt;table&amp;gt;SHOW FUNCTIONSSHOW CATALOGS [LIKE &amp;lt;pattern&amp;gt;]SHOW SCHEMAS [FROM &amp;lt;catalog&amp;gt;] [LIKE &amp;lt;pattern&amp;gt;]SHOW TABLES [FROM &amp;lt;schema&amp;gt;] [LIKE &amp;lt;pattern&amp;gt;]USE [&amp;lt;catalog&amp;gt;.]&amp;lt;schema&amp;gt;Show configured resourcesEvery SEP server is configured to connect to one or more datasources by means of a catalog that defines the connection type. Each cataloghas at least one schema; each schema has at least one table.To see the list of catalogs configured for the current server, run:presto&amp;gt; show catalogs;For the Starburst-providedDocker image, this returns:  Catalog----------- jmx memory system tpcds tpch(5 rows)(Each query also returns a set of performance metadata, which is not repeatedhere.)Explore the tpch catalogStart with the tpch catalog, which allows you to test the capabilities andquery syntax of Presto without configuring access to an external data source.The TPCH connector is described on its documentationpage.See the schemas provided in the TPCH connector:presto&amp;gt; show schemas from tpch;This returns:       Schema-------------------- information_schema sf1 sf100 sf1000 sf10000 sf100000 sf300 sf3000 sf30000 tiny(10 rows)To see the tables in one of these schemas:presto&amp;gt; show tables from tpch.sf100;Save typing with USETo avoid typing the catalog and schema name every time:presto&amp;gt; use tpch.sf100;presto:sf100&amp;gt;To see the tables in sf100:presto:sf100&amp;gt; show tables;which returns:  Table---------- customer lineitem nation orders part partsupp region supplier(8 rows)To see the structure of the customer table:presto:sf100&amp;gt; show columns from customer;which returns:   Column   |     Type     | Extra | Comment------------+--------------+-------+--------- custkey    | bigint       |       | name       | varchar(25)  |       | address    | varchar(40)  |       | nationkey  | bigint       |       | phone      | varchar(15)  |       | acctbal    | double       |       | mktsegment | varchar(10)  |       | comment    | varchar(117) |       |(8 rows)CLI with argumentsYou can submit a valid SQL script on the presto command line:presto --execute &#39;SELECT custkey, name, phone, acctbal FROM tpch.sf100.customer LIMIT 7&#39;You can send a SQL script file to the CLI from the command line by specifyingits name as an argument to the -f or --file command line options.presto -f filename.sqlTwo TPCH scripts are included with the sample files for the O’Reilly bookPresto: The DefinitiveGuide.To use these scripts, download the book’s samples from their GitHublocation either as azip file or a git clone. Let’s say you place your clone or unzip directory in~/bookfiles. Then use the TPCH scripts as follows:cd ~/bookfiles/tpchpresto -f nations.sqlThe second sample script requires the Black holeconnector to be configured for thecurrent server. This connector is designed to operate like /dev/null and/dev/zero for high performance testing of Presto components.You can configure a local Docker-hosted SEP server to use thisconnector by following the steps in Map a local etcdirectory.If the target SEP server has the Black Hole connector, then runthe second sample script:presto --file=tpch-queries.sqlTo run this second script more than once, you must stop and restart the{site.terms.sep}} server.For a locally running tar.gz installation, in the sep-root directory foryour server, run:sudo bin/launcher restartFor a locally running Docker server:docker restart sepdock   Note:  To see the Web UI in action, keep the Web UI openand visible in a browser window while the tpch-queries script runs.Further studyMore information on the CLI is available in the Command line interfacesection in the SEP referencedocumentation."
 },
 {
  "title": "Cluster setup and configuration",
  "url": "/starburst-enterprise/platform-administrator/cluster.html",
  "content": "Cluster setup and configurationStarburst Enterprise platform (SEP) is powerful and highly configurable. We have extensivedocumentation to help you ensure that SEP works as efficientlyand securely as possible in your environment.ArchitectureThe following provide a good starting place for thinking about what your clusterwill look like:  Server types  Caching modesNetworkingAs with any enterprise solution, SEP has a few networking tasksto consider:  Presto port  Coordinator high availabilitySecuringStarburst has an array of powerful, comprehensive security features toensure that your data governance and security are top-notch. We strongly suggestyou begin by watching the training video below. After that, our extensivesecurity documentation will help you get started securing your data withSEP.  Securing Starburst training video  Security documentationConfiguration basicsWhen you are ready to install, we’ve got you covered with detailed referencedocumentation covering everything from deployment to setting up data sourceconnections:  Coordinator and worker configuration  Node properties  JVM configuration  Catalog properties introduction"
 },
 {
  "title": "Clusters",
  "url": "/starburst-galaxy/clusters.html",
  "content": "ClustersA cluster in Starburst Galaxy uses Starburst Enterprise andprovides the resources to run queries against numerous datasources. Clusters define the number of workers, theconfiguration for the JVM runtime, configured data sources, and other aspects.The Starburst Galaxy platform (SGP) allows you to create, edit, and delete clustersfrom the interface. Access your clusters at any time by clicking Clusters on theleft hand menu.Add a new clusterBefore you can create a cluster in the SGP, you need toadd one or more data sources.  On the Clusters page, select + New. If this is your first cluster,you can select + New cluster from the Dashboard or Clusters pages.  Enter a unique Cluster name. Names can use lowercase letters, numbers, and hyphens.  Select your cluster profile (review cluster profile details below):          General purpose      Compute optimized      Memory optimized        From the Add data source(s) drop down menu, select the data sources for your cluster.  Confirm your deployment settings, Starburst provides default settings that you can edit at any time:          Region      Availability zone      Minimum workers (nodes)      Maximum workers (nodes)        Add tags to your cluster to organize and apply metadata. Select tags from theAdd an existing tag drop down menu or add new ones using the Create a new tag option (select Key &amp;gt; add Value &amp;gt; click Save).  Select Create cluster to finish.You can start, stop, and edit your cluster at any time from the Clusters page.Cluster profile overviewStarburst Galaxy provides three cluster profiles to allow you tocreate a cluster that is right for your purposes. Review the cluster profiledifferences:  General purpose: Balance your compute, memory, and networking resources. Suitable for a variety of diverse workloads.          Instance type: m5a.8xlarge      vCPUs: 32      Memory (GiB): 128      Instance Storage (GiB): EBS Only      Network Bandwidth (Gbps): Up to 10      EBS Bandwidth (Mbps): 4,750        Compute optimized: Ideal for compute bound applications that benefit from high performance processors.          Instance type: c5.18xlarge      vCPUs: 72      Memory (GiB): 144      Instance Storage (GiB): EBS Only      Network Bandwidth (Gbps): 25      EBS Bandwidth (Mbps): 19,000        Memory optimized: Deliver fast performance for workloads that process large data sets in memory.          Instance type: r5.8xlarge      vCPUs: 32      Memory (GiB): 256      Instance Storage (GiB): EBS Only      Network Bandwidth (Gbps): 10      EBS Bandwidth (Mbps): 6,800      "
 },


 {
  "title": "Developing custom connectors",
  "url": "/starburst-enterprise/data-engineer/custom-connectors.html",
  "content": "Developing custom connectorsStarburst Enterprise platform (SEP)  comes with an array of built-in connectors for avariety of cloud-based and on-prem data sources. Because SEP’sarchitecture fully abstracts the data sources it can connect to, compute andstorage are separated.That separation means that you can use the SEP connector serviceprovider interface (SPI) to build plugins for file systems and object stores,NoSQL stores, relational database systems, and custom services without anoff-the-shelf connector. As long as you can map data into relational conceptssuch as tables, columns, and rows, it is possible to create your ownSEP connector.To learn more, read our latest developerdocumentation."
 },
 {
  "title": "DBeaver",
  "url": "/data-consumer/clients/dbeaver.html",
  "content": "DBeaverThe open source tool DBeaver Community is a powerful SQLeditor and universal database tool. You can use it to access Starburstclusters, since it supports the JDBC driver.Use the following simple steps to access your cluster:  Get the necessary connectioninformation for your cluster  Start DBeaver  Start to create a new database connection in the Database Navigator withright-click dialog using Create - Connection, or File menu,New and select Database Connection in the dialog  In the selection dialog type PrestoSQL or locate the icon forPrestoSQL, click on it and select Next  Update Host, Port, Username and Password with your connectioninformation details and press Finish to connectNow you can browser catalog, schemas, and tables in the Database Navigator,use the SQL Editor, as well as all the other features for DBeaver."
 },
 {
  "title": "Deploying and updating Starburst Enterprise",
  "url": "/starburst-enterprise/platform-administrator/deploy-update.html",
  "content": "Deploying and updating Starburst EnterpriseCurrently, Starburst Enterprise platform (SEP) can be deployed in several ways, depending onyour organization’s infrastructure. Please follow the link most appropriate toyour environment to learn more about deploying SEP:  directly or through GCP using Helm charts  directly or through Red Hat Marketplace using our Kubernetes operator  on AWS using our AMI or using EKS  on Microsoft AzurePlease see our latest reference manual forcomprehensive documentation on installing Starburst Enterprise in yourenvironment."
 },
 {
  "title": "Disclaimers",
  "url": "/disclaimers.html",
  "content": "DisclaimersThe following information applies to this site hosted athttps://docs.starburst.io.Copyright© 2017 - 2021, Starburst Data, Inc. Starburst, and Starburst Data are registeredtrademarks of Starburst Data, Inc. All rights reserved.PrivacyThe privacy policy of Starburstapplies to all content on and usage of the site.TrademarksProduct names, other names, logos and other material used on this site areregistered trademarks of various entities including, but not limited to, thefollowing trademark owners and names:  Apache Software Foundation  Apache Hadoop, Apache Hive, Apache Kafka, and other names  Amazon  AWS, S3, Glue, EMR, and other names  Azul Systems Inc  Zulu  Docker Inc.  Docker  Google  GCP, GKE, YouTube, and other names  Microsoft  Azure, AKS, and others  Oracle  Java, JVM, OpenJDK, and other terms  The Linux Foundation  Kubernetes, Helm, Presto, Linux, and other names  Starburst Data  Starburst, Starburst Data, Starburst Galaxy, and other names"
 },
 {
  "title": "Trying Starburst Enterprise with Docker",
  "url": "/starburst-enterprise/try/docker.html",
  "content": "Trying Starburst Enterprise with DockerYou can install Starburst Enterprise platform (SEP) in a Docker container and runit on any Docker host. This is especially useful as a test or demo environmentor to evaluate SEP.With custom configuration of the containers for the SEPcoordinator and worker nodes, you can run a set of Docker containers toimplement a Linux-hosted SEP cluster on your local network or ona cloud provider.   Note:  You can run SEP with Docker on Mac or Windows computers for evaluation purposes only. Understand that these are likely to be memory-limited devices that cannot run complex queries. Production implementation of a SEP cluster on Docker is supported only on Linux servers.This page presumes you have Docker installed and running, and presumes somefamiliarity with Docker commands. For Mac and Windows evaluations, make sure youhave Docker Desktop installed and running.Initial run of SEP on DockerStarburst provides a Docker image on Docker Hub that contains a trialconfiguration of SEP. Use the following command to download andrun this image:docker run -d -p 8080:8080 --name sepdock starburstdata/presto:latestThe docker run options have the following meanings:            Option      Meaning                  –d      Detach              –p      Map ports as hostport:containerport              ‑‑name       Provide a name for the image to use in subsequent Docker commands      To make sure the server starts in Docker, continually view the Docker logs untilyou see “======== SERVER STARTED ========”.docker logs sepdockdocker logs sepdock...Use the docker ps command to verify that the your sepdock service isrunning. Run docker ps -a to locate a server that failed to start.Docker image featuresThe default SEP image has the following characteristics:  There is no Presto CLI command installed in the Docker imageitself. You can, of course, run the CLI on the host.  The jvm.config file is set to use 1G maximum  The following catalogs are installed:          jmx      memory      system      tpcds      tpch      Verify the serverTo verify that your Docker-hosted SEP server is operating asexpected, run the Presto UI as described in Verify theserver.Run queriesTo run queries against your server, use the Presto CLI as describedin CLI.Map a local etc directoryThe Starburst-provided Docker image gives you a certain set ofconfiguration files. But the whole point of running Starburst Enterprise isto query your own data sources. How do you provide your own configuration filesto the Docker-hosted server?Starburst does not recommend going into the Docker image to changeconfiguration files there. Those changes would be lost the next timeStarburst updated the public Docker image to a new version, which yournext docker run command would automatically download and use.Instead, you can map the etc directory used by the SEP instancerunning in Docker to a local directory. Once configured, the Docker-hostedSEP uses the local etc directory as its primary source ofconfiguration files, and ignores the default settings in the Docker image.To do this requires one extra docker run option. If your Docker-hostedSEP is running now, first stop and remove it:docker stop sepdockdocker rm sepdockPopulate the local etc directoryCreate a local directory to contain your custom SEP configurationfiles. For example:mkdir -p ~/sepdockThe simplest way to test your local etc directory is to use the set ofconfiguration files provided as examples for the O’Reilly book Presto: TheDefinitive Guide.Download these samples from their GitHublocation either as azip file or a git clone.Unzip or clone the files into a local directory, such as ~/bookfiles. Fromthere, copy the entire etc directory from the single-installation sample toyour ~/sepdock directory. For example:cd ~/bookfiles/single-installationrsync -av etc ~/sepdock/Edit local configuration filesNavigate to your local etc directory. For example:cd ~/sepdock/etcInspect the configuration files provided there for suitability with Docker. Forexample, the jvm.config file sets -Xmx16G, which might be too large forDocker running on Docker Desktop on Mac or Windows. Set that to a lower valuesuch as -Xmx2G.   Note:  Make sure Docker is configured to reserve enoughmemory to handle the -Xmx setting you choose for SEP. For example, DockerDesktop’s default RAM setting is 2G. Change Docker Desktop to use enough RAM toallow the Docker-hosted server to start and run.Restart to use the local etc directoryThe docker run command has another useful option:            Option      Meaning                  ‑‑volume      Specify a directory in the container to mount to a host directory      The syntax for the ‑‑volume option is:localPath:containerPath:optionsSpecify localPath first, followed by containerPath, separated by a colon. Nooptions are needed. Options are described in Dockerdocumentation.If your Docker-hosted server is running, stop and remove it:docker stop sepdockdocker rm sepdockRerun the Docker image, this time including a ‑‑volume option thatmaps the current directory, ~/sepdock/etc to the etc directory inside theDocker image, /usr/lib/presto/etc.docker run -d -p 8080:8080 --volume $PWD:/usr/lib/presto/etc --name sepdock starburstdata/presto:latestContinually view the Docker logs as before, waiting to see “======== SERVERSTARTED ========”:docker logs sepdockdocker logs sepdock...Verify that the server is running by using the Web UI as described in Verifythe server.To run queries, connect the PrestoCLI to the Docker-hostedSEP instance. Run the show catalogs;, show schemas;, andshow tables; commands to confirm the assets of the server.Configure custom data sourcesThe local etc directory feature described in the previous section lets youwork locally to make changes to your Docker environment.Continue adding new catalogs and configuring features that customize connectionsto the data sources you need SEP to see."
 },
 {
  "title": "Glossary",
  "url": "/glossary.html",
  "content": "GlossaryTerms A-EAmazon AWS marketplaceA provider for all aspects of the required infrastructure. This includes usingAWS CloudFormation for provisioning, Amazon Simple Storage Service (S3) forstorage, Amazon Machine Images (AMI), and Amazon Elastic Compute Cloud (EC2) forcomputes, Amazon Glue as metadata catalog, and others. For more information, seeAmazon AWS Marketplace.CatalogCatalogs define and name the configuration to connect to, and query a datasource. For more information, seeCatalogs.ClusterA cluster provides the resources to run queries against numerous data sources.Clusters define the number of workers, the configuration for the JVM runtime,configured data sources, and others aspects. For more information, see Clusterdeploymentand setup andconfiguration.ConnectorTransforms the underlying data into the SEP concepts of schemas,tables, columns, rows, and data types. A connector is specific to the datasource it supports, and are named as properties incatalogs.COTSCommon off-the-shelf. Refers to commodity hardware components.Data consumer personaOwns data products such as reports, dashboards, models, and the quality ofanalysis. For more information, see Starburstpersonas.Data engineer personaOwns schemas and is responsible for the source data quality and ETL SLA. Formore information, see Starburst personasData sourceA data source is a system from which data is retrieved. In SEP,you must connect to a data source so you can query that source by using acatalog. See Configure and definecatalogsTerms F-JGoogle GCP marketplaceDeploy in the Google Cloud Marketplace or using the Starburst Kubernetessolution on the Google Kubernetes Engine (GKE). GKE is a secure, productionready, managed Kubernetes service in GCP managing for containerizedapplications. For more information, see Google GCPmarketplace.Terms K-OMarketplacePurchase a preconfigured set of machine images, containers, and other neededresources to run SEP on their cloud hosts under your control. SeeMarketplace deployments.Microsoft Azure marketplaceDeploy using in the Azure Marketplace or using the Starburst Kubernetes solutiononto the Azure Kubernetes Services (AKS). AKS is a secure, production-ready,managed Kubernetes service on Azure for managing for containerized applications.For more information, see Microsoft AzureMarketplace.Terms P-TPlatform administrator personaOwns platforms and services (ITIL-style). Has service SLA responsibility for theinfrastructure supporting the cluster. For more information, see Starburstpersonas.Red Hat OpenShift marketplaceA container platform using Kubernetes operators that automates the provisioning,management and scaling of applications to any cloud platform or even on-prem.Starburst Enterprise is available on Red Hat marketplace as of OpenShift version 4.For more information, see Red HatMarketplace.Starburst Enterprise platformHelps companies harness the value of open source Presto, the fastestdistributed query engine available today. Starburst adds connectors, security,and support that meets the needs for fast data access at scale. For moreinformation, seeStarburst Enterprise.SEPAbbreviation of Starburst Enterprise platform. For more information, seeStarburst Enterprise.SQLStructured Query Language. The standard language used with relational databases.For more information, see SQL.TrinoFast distributed SQL query engine for big data analytics, formerlyPrestoSQL."
 },


 {
  "title": "Icon usage",
  "url": "/internal/icons.html",
  "content": "Persona based icons                    Platform administrator                        Data consumer                        Data engineer    Topic based icons                    Admin                        Catalog                        Clients                        Clusters                        Connectors                        Data sources                        Deploying                        Migration                        Performance tuning                        Query performance    "
 },
 {
  "title": "Data consumer user guide",
  "url": "/data-consumer/index.html",
  "content": "                    Starburst                                      Data consumer user guide      Anytime you need help with using Starburst products to drive        your day-to-day analytics, data science or business intelligence, start        here. This guide always takes you to the most up-to-date information.                              What you need to know                                                                                Introduction              Use Starburst              products with your favorite tools                                                                                                    Clients              JDBC and ODBC drivers, and tools that use them                                                                                                    Starburst SQL              The SQL language, syntax, and functions and operators                                                                                                    Migration              How to easily move your analytics to Starburst                                                      Other resources      We&#39;ve also gathered some resources on popular topics for you.                                                Query federation                                                                    Query optimizer                              "
 },
 {
  "title": "Amazon AWS marketplace",
  "url": "/marketplace/amazon/index.html",
  "content": "Amazon AWS marketplace                          Starburst Enterprise platform (SEP) is available on      AWS      marketplace.      As an alternative, you can manage SEP directly on AWS with thehelp of the Amazon Cloud Formation template and all relateddocumentation.You can also use the Amazon Elastic Kubernetes Service (EKS) directly, and findmore details in ourKubernetes documentation."
 },
 {
  "title": "User guide",
  "url": "/starburst-enterprise/index.html",
  "content": "                    Starburst Enterprise                              Get started                      Reference documentation                                                  Welcome to the user guide for Starburst Enterprise platform (SEP). We&#39;ve charted a course for you that is tailored to your duties as a data crew member. Select your role below to start your journey.                                                                              Data consumer              I use data from Starburst with my BI and data science tools to create important business insights                                                                                                    Data engineer              I provide the source data to data consumers, and ensure its quality, availability and performance                                                                                                    Platform administrator              I install and manage the clusters that serve our data,                 and ensure everything is performing well                                                Still not sure where to start? Head over to our user personas page to learn which fits you best.              Search by topic      We&#39;ve also gathered some resources on popular topics for you.                                                Tuning                                                                    Videos                                                                    Clusters                                                                    SQL                              "
 },
 {
  "title": "Client overview",
  "url": "/data-consumer/clients/index.html",
  "content": "Client overviewStarburst let’s you query all the connected data sources with the mostwidely used and best supported query language - standard SQL. You can even querymultiple data sources with the same query. Simple queries, and these morecomplex federated queries all access the data right in the source. There is noneed for ETL processes.As a data consumer you can use any supported client tool to connect to yourStarburst cluster, and execute your queries. Many tools allow you muchmore powerful usage than simply running custom written queries. You can createcomplex reports, charts, dashboards, and many other useful results.Connection informationThe details you need to connect to Starburst are independent of yourtools of choice:  URL of the Starburst cluster, including any port used  credentials, typically username and passwordAsk your Starburst platform administrator for this information.Let’s look at some examples:A simple test installation on your local computer, using the default port and noTLS configuration:  http://localhost:8080  username can be a random string like your firstname since no authorization isconfigured  no passwordThe same simple test application running on a different server:  http://starburst.example.com:8080  random username string  no passwordIf you configure TLS, you typically use a loadbalancer or proxy and the defaultport is then used, and protocol changes:  https://starburst.example.com  random username string  no passwordTLS is a requirement for added authorization, such as your LDAP directory. Inthis case you have to use real credentials:  https://starburst.example.com  LDAP username  LDAP passwordOther authorization setups require further details, and support for them variesamong client tools.Starburst client tools and driversStarburst offers a number of supported clients and tools  Command line interface (CLI) for shell scripts and manualexecution using a terminal  Java Database Connectivity (JDBC) driver, typically forJVM-based applications and others with JDBC support  Open Database Connectivity (ODBC) driver, typically for forWindows-based applications and others with OBDC support  Tableau  Microsoft Power BIThe JDBC and ODBC driver can be used for many other clients, and you can findinstructions and specifics tips for the following tools:  DBeaverOther client tools and driversIn addition, the Presto community and the wider open sourcecommunity provide numerous other clients and tools that can be used withStarburst.Other resourcesThe O’Reilly book, Presto, the DefinitiveGuide was writtenhere at Starburst, and contains some great information on gettingstarted with using different types of clients with Starburst. It’savailable for free!"
 },
 {
  "title": "Hitchhiker&#39;s Guide to editing Starburst content",
  "url": "/internal/index.html",
  "content": "Hitchhiker’s Guide to editing Starburst content  Markdown guide with all sorts of examples and usage  Our documentation iconography guide  Personas we write for  Metadata tags for page front matterBreadcrumbsThe site currently uses a simple path based breadcrumb system implemented inbreadcrumbs-simple.html.The breadcrumbs systemfrom Sean Hammond is also included as inspiration for future improvements ormigration or a merge of the two systems. It does not work for plain pages andpaths but does work nicely for posts and collections."
 },
 {
  "title": "Starburst for data engineers",
  "url": "/data-engineer/index.html",
  "content": "Starburst for data engineersStarburst Enterprise platform (SEP) is a fast, interactive distributed SQL query enginethat decouples compute from data storage. SEP lets you query datawhere it lives, including Hive, Snowflake, MySQL or even proprietary datastores. A single SEP query can combine data from all these datasources and more.Starburst can greatly reduce reliance expensive, complex and oftenbrittle ETL frameworks and their pipelines. Because it uses data instead of diskto execute queries across the cluster, it’s also fast. SEP canpull your landing times forward, and help you meet or beat your SLAs.How does this work?SEP comes with 30+ supported enterpriseconnectors includingexclusive connectors not available in open source, providing high performanceSQL-based access to most of the data platforms in your organization - such asTeradata, Postgres, and Hive. Each data platform is defined as aSEP catalog. Catalogs, in turn, define schemas and their tables.Catalogs also, at a minimum, define the connector that SEP usesto connect to that data source:connector.name=sqlserverconnection-url=jdbc:sqlserver://&amp;lt;host&amp;gt;:&amp;lt;port&amp;gt;;database=&amp;lt;database&amp;gt;connection-user=rootconnection-user=secretOnce you have your connection established, many connectors have configurationproperties that help you tune the connector’s performance, such as for timeouts,retries and connection limits.                                                          ×                                            SEP uses an ANSI-compliant SQL that should feel comfortable andfamiliar. SEP takes care of translating your queries to thecorrect SQL syntax for your data source. If you are migrating from Hive, we havea migration guide in ourdocumentation.How do I get started?If your organization does not already have SEP, you can request atrial instance, following the steps in TryingStarburst products.If you do already have SEP, as a data engineer, you need thenames and access credentials to the cluster’s coordinator and worker nodes, aswell as our JDBC orODBC driver. We also have ahandy CLI for you to use.For your next stop, see our data engineer’s userguide."
 },
 {
  "title": "Starburst Galaxy",
  "url": "/starburst-galaxy/index.html",
  "content": "                    Starburst Galaxy        Early access program                        All the benefits of        Starburst Enterprise        and much more, running in the cloud, managed for you by Starburst.                                                            Get started                                                                                Data sources                                                                                Clusters                                                      Learn about querying and using Starburst Galaxy with your BI,        reporting, or SQL tool of choice.                  Data consumer resources                    "
 },
 {
  "title": "Red Hat Marketplace",
  "url": "/marketplace/redhat/index.html",
  "content": "Red Hat Marketplace                          Starburst Enterprise platform (SEP) is available on      Red      Hat OpenShift.      OpenShift from Red Hat Marketplace (RHM) is a container platform usingKubernetes Helm charts that automate the provisioning, management, and scalingof applications to any cloud platform or even on-prem. Starburst Enterprise platform (SEP)is available onRHMas of OpenShift version 4.PrerequisitesBefore you get started, here are some things you need:  Access to an OpenShift cluster using IAM credentials, and with sufficientElastic IPs  Previously installed and configured Kubernetes, including access tokubectl  A Helm chart repository for your organization  An editor suitable for editing YAML files  Your SEP license fileFor more information, please refer to our Kubernetes withHelm documentation.Quick startAfter you have signed up through RHM, download the latest OpenShift ContainerPlatform (OCP) client for your platform from the OpenShift mirrorsite, andcopy the oc executable into your path, usually /usr/local/bin. Once thisis done, you are ready to install the operator in OCP4.Using your administrator login for Red Hat OCP, log in to the OCP web consoleand click Operators &amp;gt; OperatorHub in the left-hand menu.Once there, select “Presto” from the Project: drop-down menu, and navigatethrough the projects to All Items &amp;gt; Big Data &amp;gt; Starburst until yousee Starburst Enterprise. Click on that tile, then click the Installbutton.When the Create Operator Subscription page appears, select theStarburst project as the specific namespace on the cluster, leaveall other options as default, and click Subscribe.When the operation is complete, you are subscribed to the SEPoperator, and it is installed and accessible to you in OCP4.Getting up and runningInstallationOnce you have your operator subscription in place, it’s time to install. Thereare several steps to getting SEP installed and deployed:  Installing the SEP cluster  Installing the Hive Metastore Service (HMS)  Installing Apache Ranger (optional)You must install the HMS to connect and query any objects storage with the Hiveconnector. This is typically a core use case for SEP, and then arequired step. The HMS is used by SEP to manage the metadata ofany objects storage.Once these installations are complete, you need to assign containers to specificnodes or pods in your cluster.Read more about installation and node assignments in our Kubernetes with Helminstallation guide.ConfigurationWhen your installations are complete and the nodes are assigned containers, youmust configure your cluster. Just like with installation, there are severalsteps to configuring Starburst Enterprise:  Configuring SEP  Configuring the Hive metastore  Configuring Apache Ranger (if installed)Each of these steps uses a specific Helm chart. Click on the links for detailedinstructions on configuring each of the SEP Helm charts.Make sure to configure your desired data sources as catalogs when you configurethe SEP chart.Next stepsYour cluster is now operational! You can now connect to it with your clienttools and start querying your data sources.We’ve created an operations guide toget you started with common first steps in cluster operations.It includes some great advice about starting with a small, initial configurationthat is built upon in our cluster sizing and performance videotraining.TroubleshootingSEP is powerful, enterprise-grade software with many movingparts. As such, if you find you need help troubleshooting, here are some helpfulresources:  LDAP authentication  Data consumer guide with clients, SQL and other tipsFAQsQ: Once it’s deployed, how do I access my cluster?A: You can use the CLI on aterminal or the WebUI to access your cluster. Forexample:      Presto CLI command: ./presto --serverexample-presto-presto.apps.demo.rht-sbu.io --catalog hive        Web UI URL: http://example-presto-presto.apps.demo.rht-sbu.io        Many other client applications canbe connected, and used to run queries, created dashboards and more.  Q: I need to make administrative changes that require a shell prompt. How to I get a command line shell prompt in a container within my cluster?A: On OCP, you’ll get a shell prompt for a pod. To get a shellprompt for a pod, you’ll need the name of the pod you want to work from. To doso, log in to your cluster as per your RHM documentation. For example:oc login -u kubeadmin -p XXXXX-XXXXX-XXXXX-XXXX https://api.demo.rht-sbu.io:6443Get the list of running pods:❯ oc get pod -o wideNAME                                                 READY   STATUS    RESTARTS   AGE   IP            NODE                                         NOMINATED NODE   READINESS GATEShive-metastore-example-presto-XXXXXXXXX-lhj7l        1/1     Running   0          27m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;presto-coordinator-example-presto-XXXXXXXXX-4bzrv   1/1     Running   0          27m   10.129.2.XX   ip-10-0-153-XXX.us-west-2.compute.internal     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;presto-operator-7c4ff6dd8f-2xxrr                     1/1     Running   0          41m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;presto-worker-example-presto-XXXXXXXXX-522j8        1/1     Running   0          27m   10.131.2.XX   ip-10-0-139-XXX.us-west-2.compute.internal    &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;presto-worker-example-presto-XXXXXXXXX-kwxhr        1/1     Running   0          27m   10.130.2.XX   ip-10-0-162-XXX.us-west-2.compute.internal   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;presto-worker-example-presto-XXXXXXXXX-phlqq        1/1     Running   0          27m   10.129.2.XX   ip-10-0-153-XXX.us-west-2.compute.internal     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;The pod name is the first value in a record. Use the pod name to open ashell:❯ oc rsh presto-coordinator-example-presto-XXXXXXXXX-4bzrvA shell prompt will appear. For example, on OCP 4.4:sh-4.4$Q: Is there a way to get a shell prompt through the OCP web console?A: Yes. Log in to your OCP web console and navigate toWorkloads &amp;gt; Pods. Select the pod you want a terminal for, and click theTerminal tab.Q: I’ve added a new data source. How do I update the configuration to recognize it?A: Using the making configurationchanges section to edit your YAML configuration,find additionalCatalogs, and add an entry for your new data source. Forexample, to add a PostgreSQL data source called mydatabase:    mydatabase: |      connector.name=postgresql      connection-url=jdbc:postgresql://172.30.XX.64:5432/pgbench      connection-user=pgbench      connection-password=postgres123Once your changes are complete, click Save and then Reload to deployyour changes. Note that this restarts the coordinator and all workers on thecluster, and might take a little while."
 },
 {
  "title": "Data engineer user guide",
  "url": "/starburst-enterprise/data-engineer/index.html",
  "content": "                    Starburst Enterprise                                      Data engineer user guide      Anytime you need help configuring and securing      catalogs or developing custom connectors, start here.      This guide takes you to the most up-to-date information.                          What you need to know                                                                                          Catalogs              Defining, configuring, and securing data sources                                                                                                                Query performance              Using the query optimizer and session properties to tune SEP                                                                  Search by topic      We&#39;ve also gathered some resources on popular topics for you.                                                Migrating to SEP                                                                    Query federation                                                                    Custom connectors                                                                    SQL in Starburst                              "
 },
 {
  "title": "Platform administrator user guide",
  "url": "/starburst-enterprise/platform-administrator/index.html",
  "content": "                    Starburst Enterprise                                          Platform administrator user guide      Anytime you need help with an SEP administration         task or concept, you should start here. This guide takes         you to the most up-to-date information on running your SEP cluster.                          What you need to know                                                    Deploy and maintain your cluster          From initial install to keeping SEP up-to-date                                                            Tune your cluster          Advanced configuration for even better performance                                                              Set up and configure your cluster          Covering architecture, networking, security, and basic configuration                                Search by topic      We&#39;ve also gathered some resources on popular topics for you.                                                Security                                                                    SQL in Starburst                                                                    Query optimizer                                                                    Clients                              "
 },
 {
  "title": "Marketplace deployments",
  "url": "/marketplace/index.html",
  "content": "                    Marketplace deployments                                           Starburst Enterprise on any cloud      Many major cloud providers include      Starburst Enterprise platform (SEP)      in their      marketplace offerings. You can purchase a preconfigured machine images,      containers and other needed resources to run SEP on their      cloud hosts under your control. A free trial configuration is available      from most providers. For all marketplace implementations, you need the      following:              Access credentials from your cloud provider        The URLs and access credentials for the data sources you want your            cloud-hosted SEP cluster to access        All appropriate ACLs must be in place                                      Starburst is available in the following marketplaces:                                                                              Amazon AWS                                                                                                    Google GCP                                                                                                    Microsoft Azure                                                                                                    Red Hat OpenShift                                          "
 },
 {
  "title": "Starburst for data leaders",
  "url": "/data-leader/index.html",
  "content": "Starburst for data leadersStarburst Enterprise platform (SEP) is a fast, interactive distributed SQL query enginethat decouples compute from data storage. SEP lets you query datawhere it lives, including Hive, Snowflake, MySQL or even proprietary datastores. A single SEP query can combine data from all these datasources and more. SEP can run on-prem as well as in many cloudenvironments, and comes with 30+ supported enterpriseconnectors, providinghigh performance SQL-based access to most of the data platforms in yourorganization such as Snowflake, Postgres, and Hive.Maybe your organization relies on a single variant of SQL, or maybe they use afew. With SEP, you only need to know Starburst SQL, whichis ANSI-compliant and should feel comfortable and familiar. SEPtakes care of translating your queries to the correct SQL syntax for your datasource.SEP can greatly reduce the need for expensive and complex ETLframeworks. Because it uses memory instead of disk to execute queries across thecluster, it’s also fast, and requires a minimal disk investment. It can pullyour landing times forward, and help you meet or beat your SLAs. And, Starbursthas robust access control options for your organization, from integrating withLDAP to using your existing Ranger-managed policies.Your analysts and data scientists can connect to most of their favorite toolsusing only the SEP-specific JDBC or ODBC driver, much like theones they already use, making the transition as frictionless as possible.Want to know more? No problem! Here are some resources to answer commonquestions about SEP’s capabilities and value:  What problems does Starburst solve?  Will this help me future-proof my data ops?  How does Starburst deliver value?  Is there a comprehensive product review available?"
 },
 {
  "title": "Trying Starburst Enterprise",
  "url": "/starburst-enterprise/try/index.html",
  "content": "Trying Starburst EnterpriseStarburst makes it easy to set up trial configurations ofStarburst Enterprise platform (SEP).Up and running fastUse a local machine installation to getStarburst Enterprise running quickly as a demo or trial system for yoursite’s developers.On any Linux distro, you can:      Set up SEP as a cluster of one or moreDocker containers        Set up SEP with the tar.gzinstallation  Both installation methods can be installed without a SEP license,or to unlock the performance and security enhancements of SEP,you can request a license as described in the linked pages above.For initial evaluation only, you can use either method on recent releases ofmacOS. MacOS is explicitly not supported to run SEP servicesfor any other purpose.Consider the following ways to install and evaluate SEP,listed in ease of use order.SEP on cloud provider infrastructureYou can deploy and run Starburst Enterprise on the infrastructure of manycloud providers using their marketplaceofferings.SEP managed on a Kubernetes clusterYou can install Starburst Enterprise on a Kubernetes cluster, takingadvantage of Kubernetes abstractions to free you from the details of servers,hosts, and IP addresses.Starburst recommends using Helm charts to configureyour Kubernetes cluster as described in Kubernetes withHelm.You can install Starburst Enterprise on a private Kubernetes configuration ona local network or on a cloud provider’s Kubernetes offering, including:  Amazon Elastic Kubernetes Service (EKS)  Google Kubernetes Engine (GKE)  Microsoft Azure Kubernetes Service (AKS)  Red Hat OpenShiftYou must provide:  Access credentials from your cloud provider.  The URLs and access credentials for the data sources you want yourKubernetes-hosted SEP cluster to access.SEP with full local controlYou can install and configure a Starburst Enterprise cluster yourself on alocal network or on a cloud installation that you control.The advantage of this is that you manage and control all aspects of yourSEP cluster.The disadvantage is the same: troubleshooting cluster problems requiresknowledge of and access to your configuration settings.You can configure a local Starburst Enterprise cluster:  On a local Kubernetes cluster using Helm charts.  On a local network of individual machines without Kubernetes.To install on one or more local machines, you have several options:  For RPM-based Linux distros, use thepresto-admin tool.  For any Linux distro, set up SEP with thetar.gz installation.  For any Linux distro, set up SEP as a cluster ofDocker containers."
 },
 {
  "title": "Starburst for data platform administrators",
  "url": "/platform-administrator/index.html",
  "content": "Starburst for data platform administratorsStarburst Enterprise platform (SEP) is a fast, interactive distributed SQL query enginethat decouples compute from data storage. SEP lets you query datawhere it lives, including Hive, Snowflake, MySQL and even proprietary datastores. A single SEP query can combine data from all these datasources and more. SEP can run on-prem as well as in many cloudenvironments.SEP can greatly reduce the need for expensive and complex ETLframeworks. Because it uses memory instead of disk to execute queries across thecluster, it’s also fast. It can pull your landing times forward, and help youmeet or beat your SLAs. And, SEP has robust access controloptions for your organization, from integrating with LDAP to using yourRanger-managed policies.How does this work?SEP is a distributed system that runson COTS hardware. The coordinator parses, analyzes and plans query execution,and then distributes the query plan for processing among worker machines in thecluster. Workers use connectors specific to your data sources, such asSnowflake, Postgres, and Hive to transform queries and return data.                                                          ×                                            SEP uses ANSI-compliant SQL, and takes care of translating yourqueries to the correct SQL syntax for your data sources.SEP’s ability to federate data sources in a single query reducesyour organization’s reliance on temporary tables and more complex ETL pipelines.Because SEP query processing works in memory, your diskinvestment is light.How do I get started?You can spin up a trial instance following thesteps in Trying Starburstproducts. If you already haveSEP, you can go straight to our SEP administrator’s userguide.We also have some great training videos to get youstarted, and some articles on topics you are likely to have hard questions on:  Data architecture philosophy andapproach  Referencearchitectures  Securityguide anddeep dive  SEP and Helm  SEP administration"
 },
 {
  "title": "Microsoft Azure marketplace",
  "url": "/marketplace/microsoft/index.html",
  "content": "Microsoft Azure marketplace                        Starburst Enterprise platform (SEP) is available on Microsoft Azure.      As an alternative, you can use the Microsoft Azure Kubernetes Service (AKS)directly.More information applicable to both scenarios in available in our Kubernetesdocumentation."
 },
 {
  "title": "Data sources overview",
  "url": "/starburst-galaxy/data-sources/index.html",
  "content": "Data sources overviewYou can create a catalog by selecting a data source and configuring theconnection to various external databases and other systems.Once the data source is defined and used in your Starburst cluster, youcan query the data source by accessing the catalog and the nested schemas andtables.Initial data source setupSelect a data source below to see instructions for adding it toStarburst Galaxy:  Amazon S3  MySQL  PostgreSQLData source types and propertiesData source types define the connector that Starburst Galaxy uses forthe specific data source. The connector in turn, defines the configurationproperties to use.A number of properties are used for all data source configurations.            Property      Description                  Type      Defines the connection type of the external data source. Type is        related to the connector used to access the  data in the external data        source.              Name      The name of the data source that is also used as the name of the        catalog. This name is visible to when querying PrestoSQL and        the underlying data source.              Description      A short paragraph that provides more details about the data source        than the name alone.              Connection properties      Properties required by the connectors that enable connection to a        specified data source, such as access keys.      "
 },
 {
  "title": "Notebook manager",
  "url": "/starburst-galaxy/notebook-manager/index.html",
  "content": "Notebook managerThe Starburst Galaxy platform (SGP) notebook manager enables query management,visualizations, data source exploration, and querying your connected datasources.You can access a notebook manager for each cluster in the SGPuser interface.SQL queries are stored in notes. One or more notes are managed as a collection -a notebook. Notebooks are stored in folders.Each note contains a SQL query statement. The notes can be used to actuallyexecute queries and inspect the results in table format, or createvisualizations.Here’s a quick overview of the available feature helps you accomplish:  Query management: Create and organize your folders, notebooks and notesfor easy access and sharing.  Visualizations: Create diagrams and charts with the data from your datasources, including bar charts, time series plots, and others.  DB exploration: Browse all connected data sources, schemas, tables, andthe contained data.  Search: Search the notes of all users.  Important: The notebook manager displays in a new tab in your browserStarburst GalaxyThere are three main sections of the notebook manager:  My Notebooks: A list of all of your notebooks.  Favorites: A list of your saved notebooks for quick access. Select the icon to favorite a notebook.  History: The query logs. The history shows all of your queries whether yousaved them in a note or not."
 },
 {
  "title": "Video library",
  "url": "/videos/index.html",
  "content": "                    Video library                                      Explore the growing list of videos to learn about Starburst        products, installation, tuning, management, SQL, and more.        Use timestamps to drill down and learn about a specific topic.      Cluster sizing and performance tuningLearn how to tune and size your deployment for optimal performance.Click on the video below to view the full video in YouTube, or goto the performancetuning section to view the video in segments, with links to the relevantdocumentation.                                        Watch the video and see more details now!                    Topics:              Cluster configuration and node sizing        Memory configuration and management        Improving task concurrency and worker scheduling        Tuning your JVM configuration        Investigating queries for join order and other criteria        Tuning the cost-based optimizer            Video date: 9 September 2020      Running time: 2h15m      Securing StarburstLearn how to securely deploy Starburst at scale. Click on the video below toview the full video in YouTube, or goto the security section toview the video in segments, with links to the relevant documentation.                                    Watch the video and see more details now!                    Topics:              Authentication, including password &amp;amp; LDAP Authentication        Authorization to access your data sources        Encryption including client-to-coordinator communication        Secure communication in the cluster        Support for Kerberos        Secrets usage for configuration files including catalogs            Video date: 26 August 2020      Running time: 2h07m      Advanced SQLLearn how to run more complex and comprehensive SQL queries with Starburst.Click on the video below to view the full video in YouTube, or go toview the video in segments,with links to the relevant documentation in our SQL section.                                    Watch the video and see more details now!                    Topics:              Using JSON and other complex data types        Advanced aggregation techniques        Window functions        Array and map functions        Lambda expressions        Many other SQL functions and features            Video date: 29 July 2020      Running time: 2h14m      Query tuningLearn how Starburst executes queries to help you improve query performance.Click on the video below to view the full video in YouTube, or goto the query performance sectionto view the video in segments, with links to the relevant documentation.                                    Watch the video and see more details now!                    Topics:              Explaining the EXPLAIN        Learning how queries are analyzed and executed        Understanding what the optimizer does, including some of its limitations        Showcasing the cost-based optimizer            Video date: 12 August 2020      Running time: 2h09m      "
 },
 {
  "title": "Metadata overview",
  "url": "/starburst-galaxy/metadata/index.html",
  "content": "Metadata overviewThe Starburst Galaxy supports metadata, including metastores and tags.Add, edit, or remove:  Metastores  Tags"
 },
 {
  "title": "Knowledge base",
  "url": "/knowledge-base/index.html",
  "content": "Knowledge baseOur knowledge base articles are a collection of useful articles on a variety oftopics. The articles expand topics covered in other documentation sections, oradd entirely new material.Check back regularly to find new articles added to the list:                                  "
 },
 {
  "title": "Google Cloud Marketplace",
  "url": "/marketplace/google/index.html",
  "content": "Google Cloud Marketplace                          Starburst Enterprise platform (SEP) is available on the      Google Cloud.      As an alternative, you can use the Google Kubernetes Engine (GKE) directly.More information applicable to both scenarios in available in our Kubernetesdocumentation."
 },
 {
  "title": "Starburst",
  "url": "/index.html",
  "content": "                    Everything about Starburst products        What are you going to explore today?                              Get started with Starburst Enterprise                                Use a marketplace deployment                                &amp;nbsp;                      Confused where you landed?       Go       straight to the Starburst Enterprise reference documentation!       A lot of new useful documentation is available here, but fear not,       all the comprehensive reference documentation for Starburst Enterprise is ready and constantly improving as always.                                          What can you learn here?      No matter which Starburst product you use, and what you are trying to achieve,        you can find help on these pages:              Installing and operating a Starburst cluster on your own infrastructure        Using Starburst on public cloud provider infrastructure        Connecting data sources to query your relational database or your object storage        Using a BI tool to query data with Starburst        Writing SQL statements to get more insights from your data            Explore the site to find all this and more information in articles,      reference documentation, videos and other materials                          Which crew member best describes you?                                                                              Data consumer              You use data from Starburst with your BI and data science tools to create important business insights                                                                                                    Data engineer              You provide the source data to data consumers using SEP, and ensure its quality, availability, and performance                                                                                                    Platform administrator              You install and manage the clusters that serve the data, and ensure everything is performing well                                                Are you a data leader? You can learn more    about how Starburst products can impact your data insights, ops processes, budget, and efficiency.        Still not sure where to start? Head over to our user personas page to learn which fits you best!              Other resources                                                Video library                                                                    Reference docs                                                                    SQL                              "
 },
 {
  "title": "Starburst for data consumers",
  "url": "/data-consumer/introduction.html",
  "content": "Starburst for data consumersIf you champion data-driven decisions in your org, Starburst hasthe tools to connect you to the data you need. Starburst brings allyour data together in a single, federated environment. No more waiting for dataengineering to develop complicated ETL. The data universe is in your hands!Starburst Enterprise is a distributed SQL query engine. Maybe you know asingle variant of SQL, or maybe you know a few. Starburst’s SQL isANSI-compliant and should feel comfortable and familiar. It takes care oftranslating your queries to the correct SQL syntax for your data source. All youneed to access all your data from a myriad of sources is a single JDBC or ODBCclient in most cases, depending on your toolkit.Whether you are a data scientist or analyst delivering critical insights to thebusiness, or a developer building data-driven applications, you’ll find you caneasily query across multiple data sources, in a single query. Fast.How does this work?Data platforms in your organization such as Snowflake, Postgres, and Hive aredefined by data engineers as catalogs. Catalogs, in turn, define schemas andtheir tables.  Depending on the data access controls in place, discovering whatdata catalogs are available to you across all of your data platforms can beeasy! Even through a CLI, it’s asingle, simple query to get you started with your federated data:presto&amp;gt; SHOW CATALOGS; Catalog--------- hive_sales mysql_crm(2 rows)After that, you can easily explore schemas in a catalog with the familiar SHOWSCHEMAS command:presto&amp;gt; SHOW SCHEMAS FROM hive_sales LIKE `%rder%`; Schema--------- order_entries customer_orders(2 rows)From there, you can of course see the tables you might want to query:presto&amp;gt; SHOW TABLES FROM order_entries; Table------- orders order_items(2 rows)You might notice that even though you know from experience that some of yourdata is in MySQL and others in Hive, they all show up in the unified SHOWCATALOGS results. From here, you can simply join the data sources fromdifferent platforms as if they were from different tables. You just need to usetheir fully qualified names:SELECT    sfm.account_numberFROM    hive_sales.order_entries.orders oeoJOIN    mysql_crm.sf_history.customer_master sfmON sfm.account_number = oeo.customer_idWHERE sfm.sf_industry = `medical` AND oeo.order_total &amp;gt; 300LIMIT 2;How do I get started?The first order of business is to get the latest StarburstJDBC orODBC driver and get itinstalled. Note that even though you very likely already have a JDBC or ODBCdriver installed for your work, you do need the Starburst-specificdriver. Be careful not to install either in the same directory with other JDBCor ODBC drivers!If your data ops group has not already given you the required connectioninformation, reach out to them for the following:  the JDBC URL - jdbc:presto://example.net:8080  whether your org is using SSL to connect  the type of authentication your org is using - username or LDAPWhen you have that info and your driver is installed, you are ready to connect.What kind of tools can I use?More than likely, you can use all your current favorite tools, and even ones onyour wishlist.                                                          ×                                            Right now, Microsoft Power BI is the only BI tool that requires an additionaldriver. You can read more about thathere.How do I migrate my data sources to Starburst?In some cases, this is as easy as changing the sources in your FROM clauses.For some queries there could be slight differences between your data sources’native SQL and SQL, so some minor query editing is required. Rather thanchanging these production queries on the fly, we suggest using your favorite SQLclient or our ownCLI to test yourexisting queries before making changes to production.If you are migrating from Hive, we have a migrationguide in ourdocumentation. To help you learn how others have made the switch, here is ahandy walk-through of usingLookerand Starburst Enterprise together.Where can I learn more about Starburst?From our documentation, of course! Visit our data consumer’s user guide."
 },
 {
  "title": "JDBC driver",
  "url": "/data-consumer/clients/jdbc.html",
  "content": "JDBC driverThe Java Database Connectivity (JDBC) driver enables any application supportinga JDBC driver to connect to Starburst clusters. The application can thenissue SQL queries and receive results.Typically the applications are JVM-based, or support JDBC driver usage with adifferent mechanism. Using an applications with the JDBC driver generallyfollows the same steps.Connection informationGet the necessary connection informationfor your cluster.Download and configure the driverTypically you have to download the JDBCdriver and add it to your application.Adding it to your application varies for each application. Refer to it’sdocumentation.Some applications, such as DBeaver, automatically download andconfigure the driver.Create the connection to the clusterWith a configured driver you can now configure a connections to a cluster.Typically you only need the connection information. Other details such as theJDBC URL are automatically configured in many applications, or can be seen andupdated in an user interface dialog.If your application does not include these out of the box, simply check thereference documentation for the JDBCdriver for the correct values.Start queryingThe configured connection, can now be opened and you can start running queries.For example, if you application allows you to write and run queries you can seewhat catalogs are available:SHOW CATALOGS;Many applications provide a user interface to see the list of catalogs, schema,tables and much more.Next stepsNow you can take advantage of the features of your application, and potentiallylearning more about the SQL support in Starburst."
 },
 {
  "title": "Trying SEP on Kubernetes",
  "url": "/starburst-enterprise/try/k8s.html",
  "content": "Trying SEP on Kubernetesusing the Helm charts on Kubernetes clusteron local workstation using kind ?"
 },

 {
  "title": "LDAP authentication",
  "url": "/starburst-galaxy/ldap-auth.html",
  "content": "LDAP authenticationThe LDAP configuration enables users to authenticate to theStarburst Galaxy platform (SGP).It also allows you to use the same credentials to connect with client tools,such as the PrestoSQL CLI, or an application using the JDBC or ODBCdriver. Once connected you can use them to run queries against the data sourcesconfigured as catalogs.The following properties need to be configured for your LDAP authenticationssupport. Work with the administrator of your LDAP server to determine thecorrect values.      Property    Description        URL    The combination of the protocol, FQDN, and port used by your LDAP server.      Protocol can be ldaps:// or ldap://        User bind pattern    The pattern used to look up the relevant users in your LDAP directory.      An example is ${USER}@ldap.example.com        Group authorization query    Queries the LDAP user groups        Distinguished name    Starburst Galaxy service user identity        Bind password    Authorizes the user bind pattern          User distinguished name    Service user identity        Cache TTL    Time interval to cache credentials after first authentication, and      before authentication is required again  Consult the LDAP documentation forSEP for more details,and work with your LDAP server administrator.Edit LDAP cluster settingsEdit your cluster’s LDAP settings from the Clusters page.  Locate the cluster whose LDAP settings you want to edit  Select the Options icon (three stacked dots), click Edit  Select the LDAP configuration tab  In the Configure LDAP properties section, edit your properties  When you complete your edits, click Update to save your new configuration  To apply your changes, restart your cluster"
 },
 {
  "title": "Markdown usage",
  "url": "/internal/markdown.html",
  "content": "Markdown usageThis page shows the output of all the supported markdown syntax usage. The siteuses CommonMark.Markdown source files  Standard extension is .md  80 character hard wrap width for markdown code  Embedded HTML can be wider  Use 2 space indent for HTML (no tab, not wider)Section titlesTitles are marked with one or more #.Before title..Level 1after title and before next one, should only be used for page titleLevel 2after title and before next oneLevel 3after title and before next oneLevel 4after title and before next oneLevel 5after title and before next one, too deep, probably should never be usedLevel 6after title and before next one, too deep, probably should never be usedText formattingNormal text in a paragraph. You can just write along. Spaces between word orline breaks don’t matter. An empty line starts a new paragraph.Use _ or * to highlight words in italics text. Use __ or ** tohighlight words in bolded text. Don’t mix bolding and italics.  bold with underscore  bold with double asterisk  italics with underscore  italics with asteriskStandard usage at Starburst is to use asterisk *bold*.Code blocks and other source codeInline usage of source code uses simple backticks to surround the variable orsimple command:Add ~/bin to the PATH with EXPORT PATH=~/bin:$PATH.Separate code block with three backticks:cd /opt/dev/myprojectmvn clean installYou can declare a language like shell after the initial three backticks toget syntax highlighting.Here is a shell fenced block:cd /opt/dev/myprojectmvn clean installA java fenced block:String a = String.valueOf(2);   //integer to numeric stringint i = Integer.parseInt(a); //numeric string to an intA yaml fenced block:image:  repository: &quot;harbor.starburstdata.net/starburstdata/hive&quot;  tag: &quot;338.2.2-rc.1-SNAPSHOT&quot;  pullPolicy: &quot;IfNotPresent&quot;expose:  type: &quot;clusterIp&quot;  clusterIp:    name: &quot;hive&quot;    ports:      http:        port: 9083  nodePort:    name: &quot;hive&quot;    ports:      http:        port: 9083        nodePort: 30083A sql fenced block:SELECT ARRAY[4, 5, 6] AS integers,       ARRAY[&#39;hello&#39;, &#39;world&#39;] AS varchars; integers  |   varchars-----------+---------------- [4, 5, 6] | [hello, world]If for some reason you really must have line numbers, then you must use a liquidhighlight block. This java highlight block uses the linenos keyword todisplay line numbers:12String a = String.valueOf(2);   //integer to numeric stringint i = Integer.parseInt(a); //numeric string to an intHowever, if you find yourself needing to refer to line numbers, your codeblockis likely too long for your purpose. Consider instead a one- to two-line excerptto illustrate what you are writing about, even if you keep the longer codeblockintact.Unordered listsUse - or * for the items.  Apple  Pear  BananaYou can also indent:  Pets          Dog      Cat        Farm animal          Cow      Sheep      Ordered listsThe actual numbers in your document’s source code aren’t used as the line numbervalues in static documents. The line numbers you see in the generated documentsare  computed. So you can use e.g. 1. all the time. This example uses 1. for alllines in the source document, but the generated document are numbered correctly:  Item 1  Item 2          Nested 1 (nested items in lists require 4 spaces to indent properly)      Nested 2        Item 3Definition listsUse dl/dt/dd HTML:  Term 1  the description for term 1  Term 2  the description for term 2Links  Markdown syntax with [text](url) is preferred  Relative links are preferred to avoid issues with the root context  Ideally use (/file.html  for absolute linksImagesWith markdown syntax ![title](/assets/img/image.png }}):Or HTML syntax. This allows you to add more tweaks such as sizing. Please useclass=img-center unless you have a very good reason not to. In fact, use HTMLpreferentially for this reason:You can use a size-constrained image as button to expand and see it in a largermodal. To do this, you’ll need to first create the url string in a variableusing the capture tag, as you cannot use curly braces in parameters. Then, youinclude modal.html in a separate tag block with parameters, using that urlstring in one of the parameters:  url=url_string - use relative URL to the asset  img-id - must be unique per page  alt-text - img alt tag content  descr-text - img description tag content for accessibility support  pxwidth - number only (no “px” suffix) of the thumbnail widthFeel free to cut and paste the capture and include tag blocks the display thefollowing image:                                                          ×                                            TablesDirect layout markup:            Tables      Are      Cool                  col 3 is      right-aligned      $1600              col 2 is      centered      $12              zebra stripes      are neat      $1      Semantic markup:            Markdown      Less      Pretty                  Still      renders      nicely              1      2      3      Raw HTML:            First name      Last name                  Alison      Loney              Manfred      Moser      IconsYou can and should use fontawsome icons.There are thousands available.Syntax is &amp;lt;i class=&quot;fa fa-user&quot;&amp;gt;&amp;lt;/i&amp;gt;.You can change size, color and more.This is a user icon  in a paragraph.Also be careful, we use fontawesome 5 and therefore some icons are in adifferent name space.  &amp;lt;i class=&quot;fab fa-twitter&quot;&amp;gt;&amp;lt;/i&amp;gt; for   &amp;lt;i class=&quot;fab fa-youtube&quot;&amp;gt;&amp;lt;/i&amp;gt; for   &amp;lt;i class=&quot;fab fa-linkedin&quot;&amp;gt;&amp;lt;/i&amp;gt; for   &amp;lt;i class=&quot;fab fa-github&quot;&amp;gt;&amp;lt;/i&amp;gt; for Coloring can be done with CSS or an embedded style.  &amp;lt;i class=&quot;fab fa-github&quot; style=&quot;color: red;&quot;&amp;gt;&amp;lt;/i&amp;gt; for VideosEmbedding YouTube videos is supported courtesyhttps://github.com/nathancy/jekyll-embed-video. video-embed.css is included tomake it responsive, but that is untested. Other sources can be supported, butYouTube is all we need for now.You must include the video ID you want to use in the front matter.---youtubeId1: buqtdpuZxvk---You can have multiple videos on a page, you just have to give them differentnames in the front matter.If you want to use an entire video, just use youtubePlayer.html in your include,with the :include youtubePlayer.htmlIf instead you want to play just a portion, use youtubeSnippet.html in yourinclude, and provide start and end values in seconds after the video id:id=page.youtubeId1 start=22 end=31At this point, you might be wondering to yourself, “But what if I want to startat a particular place, then play until the end?” Great question! In that case,just set the value for end to -1:id=page.youtubeId1 start=22 end=-1You’ll have to read the .md file for now to see the entire include, as it justrenders the included HTML as html in a comment:          AdmonitionsThe simplest admonition is just using a blockquote.  This is a simple blockquote.And after this insightful blockquote, you can have a look at a even biggerblockquote.  And here is a longer one.  Its has two sentences. And also      a list item    another item  Beyond that you can use specific warning, caution and note admonitions.As per Google developer documentation(GDD) a warning is: “Stronger thana Caution; it means “Don’t do this.”Note that multi-line, paragraph-shaped passages work in the styling:   Warning:Like a red morn that ever yet betokened,Wreck to the seaman, tempest to the field,Sorrow to the shepherds, woe unto the birds,Gusts and foul flaws to herdmen and to herds.William ShakespeareA caution, which, as per GDD “Suggests proceeding with caution”:   Caution:  Do not be tricked into thinking that there areno crocodiles just because the water is still. - Malaysian proverbA note (GDD: “An ordinary note or tip”):   Note:  Try using a small LIMIT when testing out a newquery.A note with a long link to see if it breaks/wraps:   Note:  This is an example note to see how the line breakis handled by css.https://starburstdata.atlassian.net/wiki/spaces/PM/pages/1080688861/Starburst+Galaxy+UX+Research-let+saddareallylong+linkYou can also use capture to create a large section ofcontentand then pass that to the admonition.Side navigationTo include multiple levels of navigation you will need to use the following ymlsyntax:nav:  - title: SEP Overview    context: /starburst-enterprise/index  - title: Platform administrators    id: admins    collapse: /starburst-enterprise/platform-administrator    subnavigation:      - title: Home        context: /starburst-enterprise/platform-administrator/index      - title: SEP clusters        id: test        collapse: /starburst-enterprise/platform-administrator/cluster        subfolderitems:         - title: Test           context: /starburst-enterprise/platform-administrator/cluster/test"
 },
 {
  "title": "Metabase",
  "url": "/data-consumer/clients/metabase.html",
  "content": "Metabase"
 },
 {
  "title": "Metastore overview",
  "url": "/starburst-galaxy/metadata/metastores.html",
  "content": "Metastore overviewThere are two places in the Starburst Galaxy platform (SGP) where you can addmetastores:  Metadata page  Data sources pageAdd metastore from the Metadata pageAdd an AWS Glue metastore or an external Hive metastore starting from theMetadata page.Add an AWS Glue metastoreAdding an AWS Glue metastore connection details allows access to metadata andmapping information about the objects stored in AWS to SGP.  Click Admin and select Metadata.  In the Metastores section, click + New.  Select the AWS Glue metastore location.  Enter a Name for your AWS Glue metastore.  Select the AWS Glue region from the drop down menu.  Select Secret key or Assumed IAM role.          Secret key: Enter the Access key ID and your Secret access key name. The secret key ID is the name of your secret key in your secret manager.      Assumed IAM role: Enter the IAM role ARN.        Click Finish to save and return to the Metadata page.Review metastore information on the Metadata page.Add an external Hive metastoreAdding an external Hive metastore connection details allows access to metadataand mapping information about the objects stored in AWS to SGP.  Click Admin and select Metadata.  In the Metastores section, click + New.  Select the External Hive metastore metastore location.  Enter the Hive metastore URI.  Click Show authentication options to add other authentication details, such as:          Hive metastore username.      Thrift client SSL enable (check the True box).      Thrift client SSL trust certificate.      Thrift client SSL trust certificate password.        Click Next.Review metastore information on the Metadata page.Add a new metastore while adding a data sourceInstead of adding a metastore from the Metadata page, add one while addinga data source SGP.  Add an AWS Glue metastore  Add an external Hive metastoreLearn about AWS Glue metastoresand external Hive metastores."
 },
 {
  "title": "Migrating your analytics to Starburst",
  "url": "/data-consumer/migration.html",
  "content": "Migrating your analytics to StarburstIn some cases, migrating your analytics to Starburst is as easy asswapping out your client, such as a JDBC driver, and changing the sources inyour FROM clauses. But because not every data source implements the SQLstandard the same, there could be slight differences between your data sources’native SQL and Starburst’s SQL. The information on this page will getyou started moving your tools and workflows to Starburst.Getting and installing Starburst clientsStarburst has JDBC and ODBC drivers to connect your favorite tools to yourfavorite data. We have an entire documentsection that covers downloading, installing andconnecting clients.Migrating queries from ANSI-standard SQL implementationsFor SQL implementations that follow the ANSI standard closely, only minor queryedits are likely. Rather than changing these production queries on the flythough, we suggest using your favorite SQL client or our ownCLI to test your existing queriesbefore making changes to production.Our full Starburst SQL referencemanual is available to help youresolve any small implementation differences as you migrate your queries toStarburst.Migrating queries from HiveIn other cases where a SQL implementation deviates significantly from the ANSIstandard, such as with Hive’s HiveQL, there some things you’ll want tokeep in mind. Our documentation covers the syntactic and semantic differencesbetween HiveQL and other non-ANSI standard implementations such as:  Array syntax and handling  Syntax for strings and identifiers  CAST considerations  Differences in datediff  Complex expressions and subqueries  INSERT and OVERWRITE operationsRead the Hive migration page in ourreference documentation for detailed information on these topics.Reducing ETLWith Starburst, there are many opportunities to both reduce yourreliance on increasingly complex ETL pipelines and the intermediate storage tocentralize data that you must transform and clean up tech debt while you do so.These are complex problems and it can be hard to know where to begin. Thefollowing sections offer some strategies that make approaching the problemeasier.Reduce or remove duplicative or similar processingIf you have multiple pipelines that ultimately produce the similar data with thesame grain, start there by combining those queries. For example, if youcalculate budget for ads at an ad_id level, and someone else is calculatinginvoices for the same ad_id grain, combine those queries. You save the doublecompute and storage demands so that you don’t scan the data twice, then move itaround twice, and finally write it twice with just one column being different.Optimize around end usageAs you are doing your due diligence for this migration, whether you choose to leave some pipelines in your current framework or not, take the opportunity to review your pipeline design against their purpose:  For end usage purely for downstream ETL, optimize for writes  For end usage for dashboards and reports, optimize for readsFor in-depth information on optimizing queries, dive into our trainingvideo onoptimizing query performance, and reference the queryoptimizer section of our referencedocumentation.Reduce or remove the need for temp tables and intermediate disk storageData products that are derived from disparate sources often need to land in anintermediate schema where they can be combined locally. With Starburst,you can simplify this type of processing and remove the need for temp tables andschemas by taking advantage of Starburst’s powerful query federationabilities. This has the additional, positive side effect of obviating the needfor managing temp table cleanup jobs, too.Reduce or remove little-used queries and data productsIf you are not already, start measuring the usage for dashboards and reports. Have any been abandoned? Has the usage shrunk enough on any of them so that there is no longer a justifiable ROI on maintaining them? If so, work with your stakeholders on a sunset plan, and remove the surfaces and pipelines.Next stepsOnce you have cleaned up your pipelines, start looking at where you can delightyour customers with more aggressive data landing times. Upstream data ETL andavailability will continue to set a lower bound on your landing times, but thereis a positive, domino effect from reducing pipeline complexity and the needto wait for slow compute and storage operations."
 },
 {
  "title": "ODBC driver",
  "url": "/data-consumer/clients/odbc.html",
  "content": "ODBC driverThe Open Database Connectivity (ODBC) driver enables any application supportinga ODBC driver to connect to Starburst clusters. The application can thenissue SQL queries and receive results.Typically the applications are Windows-based, or support ODBC driver usage on other operating systems. Using an applications with the ODBC driver generallyfollows the same steps.Connection informationGet the necessary connection informationfor your cluster.Download and configure the driverUnlike the ODBC driver, the ODBC driver requires a license. Get a licensed copyof the driver installation package from the Starburst support team.You can find more information in the ODBC driver referencedocumentation.Typically you have to install the OBDC driver and add it to your application.Adding it to your application varies for each application. Some applications,such as Power BI include the driver. Refer to thedocumentation for details for your application.Create the connection to the clusterWith a configured driver you can now configure a connections to a cluster.Typically you only need the connection information. Other details such as theODBC connection are automatically configured in many applications, or can beseen and updated in an user interface dialog.Start queryingThe configured connection, can now be opened and you can start running queries.For example, if you application allows you to write and run queries you can seewhat catalogs are available:SHOW CATALOGS;Many applications provide a user interface to see the list of catalogs, schema,tables and much more.Next stepsNow you can take advantage of the features of your application, and potentiallylearning more about the SQL support in Starburst."
 },
 {
  "title": "Tuning your cluster performance",
  "url": "/starburst-enterprise/platform-administrator/performance-tuning.html",
  "content": "Tuning your cluster performanceStarburst Enterprise platform (SEP) is a more feature-rich version ofPrestoSQL providing enhanced query performance, security,connectivity, and ease of use.Learn how to size your cluster and the machines in it to ensure the bestperformance possible for your workload in this training video presented by oneof our founders, Dain Sundstrom. For your convenience, we’ve divided the videotraining course up into topic sections, and provided links to the relevant partsof our documentation below.General tuning strategy &amp;amp; baseline advice                                    Topics:              Starting big        Stabilizing, then tuning        Options to disable            Running time: ~9 min.      Cluster sizing, and how SEP uses CPU and memory resources                                         Topics:              How memory affects JOIN, GROUP BY, ORDER BY and window functions        Availability        Concurrency            Running time: ~19 min.      Machine sizing and its impact                                     Topics:              Memory and memory allocation        Shared join hash        Distributed join        Skew        Machine sizes and types        Spilling        Small clusters            Running time: ~38 min.      Additional resources on memory management and spilling in SEP:  Memory management properties  Memory configuration  Spilling properties  Spill to disk  JVM SettingsTuning the workload                                    Topics:              Query plan        Precomputing        Connectors            Running time: ~16 min.      Hive data organization                                     Topics:              Organize your data for the Hive connector        Hive partitioning and bucketing        ORC and Parquet        File size        Bad parquet files        Rewrite table with the ORC writer            Running time: ~16 min.      Making queries faster                                     Topics:              What to look for in a query        Using more hardware        Underutilization        Hive caching            Running time: ~13 min.      For more in-depth information on this topic, watch our query optimization trainingvideo.Sharing resources, and resource groups                                     Topics:              Concurrency        User experience, expectations and satisfaction        Social engineering            Running time: ~3 min.      "
 },
 {
  "title": "Personas used for documentation and design",
  "url": "/internal/personas.html",
  "content": "Personas used for documentation and designThere are three primary audiences we write and design for:  Starburst platform administrators  Data engineers  Data consumers: analysts &amp;amp; scientistsThere is a single, secondary audience: “Data leaders.” This persona is largely acheck writer for our purposes (VP, CDO, CIO), and does not actually use SEP anydifferently than the primary personas. They are included here for completeness,and should be kept in mind when creating marketing content such as case studies,white papers and ROI-focused materials.This document describes these audiences as personas - fictional amalgamations -that you can empathize with and solve problems on behalf of. In practice,product personas are given names, faces and backgrounds to aid in discussingtheir needs as if they were real people, representative of our customers.Primary personasData consumers - analysts and scientistsData analysts and scientists will approach Starburst in very similar ways.However, their backgrounds and skill sets are different, so we will separatethose out. Their pain points will be treated together.              Chris Consumer    Early career    BSc in physics from University of Toledo, currently working on online MBA    Chris is a Business Analyst. He&#39;s responsible for delivering    visualizations and reports to ensure that his leadership is making    well-informed, data-driven decisions. Chris cares very deeply that not only    are the right questions being asked (and answered), but that the right    data is being used to answer the questions. With the wealth of data    available, it can be easy to overlook and misuse data. The quality of    Chris&#39;s work ultimately rests on the quality and reliability of the data he    uses, so Chris keeps good working relationships with his data engineering    team and often communicates discrepancies and SLAs issues to them. Chris has    some solid SQL chops and is often able to prototype a new data source to be    productionalized by data engineers. Chris feels that he has just the right    combination of technical skills and business acumen.  When you write, design and build for Chris, here are some of the skillsets you can expect him to have:  A reasonable level of skill with SQL, with some knowledge of more advancedqueries  Limited programming skills and methodologies  Tells stories with data  Expert with data visualization tools  Excellent spreadsheet skills, including some modeling  A good ability to detect and articulate issues with data, even if he cannotremedy them or trace the cause              Cameron Consumer    Early career    PhD in statistics, Stanford    Cameron is a data scientist. She&#39;s responsible for creating data models    and that forecast and describe the business. Cameron worries about the impact    of seasonality on sales, and feels compelled to deliver models that    reflect that impact with a high degree of accuracy. Cameron feels like she    brings the answers to &quot;Why?&quot; and &quot;How?&quot; to the table. Her machine learning    models help her find the levers that the business can pull - the &quot;how,&quot;    and her models account for why the business behaved as it did, or will.    She feels more like an academic than an engineer, and is very proud of her    scientific approach to business. Her digital sales data knowledge is    formidable, and her reputation as an SME ensures that she has a robust    stream of opportunities in her field.  When you write, design and build for Cameron, here are some of the skillsets you can expect her to have:  A reasonable level of SQL skills, with some knowledge of more advanced queries  Reasonable programming skills  Expert in statistical methods and/or machine learning  Some understanding of code repositories  Competence with data visualization tools  A good ability to detect and articulate issues with data, even if they cannotremedy them or trace the causeCameron’s and Chris’s pain points include, in no particular order:  Having to retrofit tools onto multiple data sources  Long waits for ETL to deliver useable data  Can’t dive into data quality issues  Data engineers sometimes kill their queries because of resource contention  Complex, periodic reports and models are often delayed past due datesData engineers                Donna Data Engineer      Mid-career      BSc in computer engineering, University of Illinois at Chicago      Donna Data Engineer is responsible for designing performant data      sources that can answer a broad range of business questions at XYZ, Inc.      Donna found her way to data engineering through internships in college; it      felt like a good blend between the technical chops required for      programming jobs, and the big picture, organizational nature of data that      she is naturally drawn to. As part of her job, she must understand what      data is currently available from what sources, and what new data is needed      to fill in any gaps. Donna has to work with stakeholders to source that      new data, be it from third parties or through new log entries, message      streams or product endpoints. Donna works pretty closely with data      analysts and scientists, and tries to anticipate their needs in order to      keep up with burgeoning data demands.                  Daniel Data Engineer      Mid-career      BSc in computer science, University of New Hampshire      Daniel Data Engineer is responsible for delivering data to data      analysts and data scientists at Acme Corp. Up until a few years ago,      this mostly entailed writing complex ETL in frameworks such as Informatica      and Alteryx. Over the last few years, he&#39;s worked mostly in python-based      frameworks such as Airflow and Bonobo as well as diving into Apache Spark.      Daniel really cares about data landing times, because him and his      coworkers hear from PagerDuty way more than they would like to.   When you write, design and build for Donna and Daniel, here are some of theskill sets you can expect them to have:  Creating and monitoring pipeline health metrics to ensure SLAs are met.  Enabling automated self-service pipelines using Infrastructure as Code (IaC)  Design schemas, data lake and data warehouse solutions in collaboration withstakeholders.  Building and managing Kafka-based streaming data pipelines  Building and managing Airflow- and Spark-based ETLs  Creating and updating data models &amp;amp; data schemas that reduce system complexityand cost, and increase efficiency  Preparing and cleaning data for prescriptive and predictive modeling anddescriptive analytics  Identifying, designing, and implementing internal process improvements such asautomating manual processes, optimizing data delivery for greater scalability  Creating data tools for analysts and data scientists  Building data integrations between various 3rd party systems such as Salesforceand WorkdayDonna’s and Daniel’s pain points, in no particular order:  Keeping up with the changing landscape of data delivery technology  Managing SLAs for data pipelines in environments where the data growth rateand complexity constantly increases, data pipeline and platform performance  Aligning and negotiating with upstream data sources and infrastructure SLAowners  Sussing out detailed data requirements from folks with a wide range of dataknowledge  Long, brittle pipelines  Productionalizing non-performant analyst queries  Constantly responding to resource constraint issues  Designing ETL around siloed data  Data cleansingPlatform administrators              Art Administrator    Late career    BSc in computer science, BYU    Art Administrator is responsible for XYZ, Inc&#39;s Starburst cluster. He was    an SRE for the data team for years, and switched roles to platform    engineering after leading the SREs for a bit. Art really cares about    scalability and reliability, especially since XYZ has super aggressive    SLAs both on data landing times and of course availability. Art works    closely with his colleagues in IT to ensure that his systems adhere to    XYZ&#39;s strict access policies and support audit requirements.                   Ada Administrator      Late career      BSc in computer science, University of Washington      Ada Administrator is responsible for both Acme Corp&#39;s Starburst and      Postgres clusters. Ada was a DBA from early to mid-career, and it fell to      her at Acme to figure out the HDFS ecosystem when it came along. Now she      builds and maintains big data clusters for a living. Ada cares a lot      about the using right data platform for the data.  When you write, design and build for Art and Ada, here are some of the skillsets you can expect them to have:  Building and maintaining scalable data platform architectures to support theingest, storage and querying of large heterogenous datasets  Creating and monitoring cluster health metrics to ensure optimal performanceand reduce any downtime  Writing clean, production-ready code (in Java, Go etc.) with a strong focus onquality, scalability and high performance  Using and building scalable asynchronous REST API’s  Working with cloud providers like AWS, Azure and Google Cloud  Implementing and working with persistence technologies like AWS S3, HDFS,Kafka and ElasticSearch  Designing for data integrity and security through all environments as well asthe data lifecycle  Partnering with data engineers to enable automated self-service pipelinesusing Infrastructure as Code (IaC)  Partnering with data engineers to design and improvement schemas, data lakeand data warehouse solutions in collaboration with stakeholdersArt’s &amp;amp; Ada’s pain points, in no particular order:  Sorting through an overload of information to master complex data platforms  Ensuring data platforms can scale to demand and with growth  Architecting solutions that can provide disaster recovery and businesscontinuity for complex, critical data systems, in conjunction with ITstakeholders  Assisting in managing budgets and licensing cycles for massiveenterprise-scale software vendors, bandwidth and hardware leases  Constantly tackling inherently complex and highly-visible tasks  Delivering against stringent infrastructure SLAs  Doing more with less, or at least the same team size  Implementing data governance requirements for all data systemsSecondary persona - data leader              Lauren Leader    Mid-to-late career    MBA, Haas School of Business    Lauren is CIO at the newly IPO&#39;ed Clouds &#39;R Us. She&#39;s responsible for    data infrastructure, data governance and delivery, as well as    enabling SOX, GDPR and CCPA compliance. Prior to stepping into her current    role, Lauren was a VP of IT at Acme Corp., where she owned the budget for    all data infrastructure. She calls this her &quot;real-life MBA,&quot; because she    learned the hard way from being caught off-guard by explosive growth in    under-specified legacy systems in multiple budget cycles. Lauren is also    sensitive to scaling, platform lock-in, and staffing around particular    technologies.  When you write for Lauren, here are some of her pain points to keep in mind, inno particular order:  Constantly fighting Shadow IT, up to and including small, narrow-scopeone-off data warehouse solutions which she inevitably must absorb  Architecting around legacy systems, particularly monolithic services  Changing regulatory climate  Staffing for innovation while keeping legacy systems running and trying toautomate  Managing, defending and demanding a budget with rapid growth  Balancing buy-vs-build, including for contracting services  Balancing private cloud vs hosted cloud solutions for cost-effectiveness,regulatory compliance and security"
 },
 {
  "title": "Microsoft Power BI",
  "url": "/data-consumer/clients/powerbi.html",
  "content": "Microsoft Power BIYou can use the popular analytics platform Microsoft PowerBI with your Starburst cluster.You can use Microsoft Power BI Desktop and Microsoft Power BI Service with theStarburst Power BI DirectQuery driver on top of the ODBC driver toconnect to your cluster.Read more about the specific details in Starburst Power BI DirectQuery driverreference documentation."
 },
 {
  "title": "Introduction to query federation",
  "url": "/data-consumer/query-federation.html",
  "content": "Introduction to query federationStarburst Enterprise is the world’s fastest distributed SQL query engine. It letsdata consumers query anything, anywhere, and get the data they need in a singlequery, no matter where it lives. This idea of combining data from disparatesources is called query federation. It allows you to combine, for instance,historical data from HDFS or objects stores with the most recent incoming datafrom Kafka one query.Federating data is simple. You just need to use the fully-qualified name of thetables in your FROM clause. Table names are fully-qualified when theyinclude the catalog name:&amp;lt;catalog&amp;gt;.&amp;lt;schema&amp;gt;.&amp;lt;table&amp;gt;A catalog defines the schemas in a data source such as Snowflake,Oracle and Hive.Here’s an example of data from two different sources, Hive and MySQL, federatedinto a single query:SELECT    sfm.account_numberFROM    hive_sales.order_entries.orders oeoJOIN    mysql_crm.sf_history.customer_master sfmON sfm.account_number = oeo.customer_idWHERE sfm.sf_industry = `medical` AND oeo.order_total &amp;gt; 300LIMIT 2;This query uses data from the following sources:  The orders table in the order_entries schema defined in the hive_sales catalog  The customer_master table in the sf_history schema defined in the mysql_crm catalogTo help you learn how Starburst uses federated queries in popularanalytics tools, here is a handy walk-through of federating queries for rapidvisualization inLookerwith Starburst Enterprise."
 },
 {
  "title": "Optimizing query performance",
  "url": "/starburst-enterprise/data-engineer/query-performance.html",
  "content": "Optimizing query performanceStarburst Enterprise platform (SEP) is fast. But did you know that there are stillmany opportunities to make it even faster depending on how you write yourqueries?Learn how to use EXPLAIN and ANALYZE to improve your query performance inthis training video presented by one of our founders, Martin Traverso. For yourconvenience, we’ve divided the video training course up into topic sections, andprovided links to the relevant parts of our documentation below.The query lifecycleKnowing what’s happening under the hood in SQL can help you to write queriesthat capitalize on possible optimizations and avoid approaches that will costyou performance. This section provides an overview of what happens as a query isexecuted.                                     Topics:              Parsing        Analysis        Planning        Optimization        Scheduling and execution            Running time: ~12 min.      The EXPLAIN statement in detailIf you want to understand what the SEP engine is basing its decisions onas it executes a query, you need to use the EXPLAIN statement. This sectionwalks you through this very informative tool in detail.                                     Topics:              EXPLAIN        EXPLAIN vs EXPLAIN ANALYZE        Fragment            structure, distribution, row layout, estimates, and performance stats in EXPLAIN ANALYZE        Exchanges            Click the links to read more on that topic in our reference manual.      Running time: ~20 min.      General optimizationsThe content in this section is more technique-oriented, and is a complexsubject. We strongly suggest watching it all the way through thoroughly first togain a broad awareness of how you write a query can affect its performancebefore trying these on your own. For further reading, we recommend ourpushdowndocumentation.The SQL engine relies on table statistics to make decisions on optimizations.Enabling dynamic filtering can take optimizations even further. We recommendreading about these powerful features to ensure you are getting the bestperformance possible out of your cluster:  Dynamic filtering  Table statistics                                         Topics:              Constant folding        Predicate pushdown        Predicate pushdown into the Hive connector        Hive partition pruning        Hive bucket pruning        Row group skipping for ORC and Parquet        Limit, partial limit, and aggregation pushdown        Skew            Running time: ~58 min.      SEP offers several properties to control how theoptimizer handles certainoperations.Cost-based optimizationsThis section presents on overview of how cost-based optimizations work in SEP,and provides great context for the following recommended reading:  Cost-based optimizations  Cost in EXPLAIN                                     Topics:              Partitioned and broadcast joins        Disabling cost-based optimizations        Join reordering        Table statistics        Computing statistics with ANALYZE            Running time: ~13 min.      "
 },

 {
  "title": "Securing Starburst Enterprise",
  "url": "/starburst-enterprise/platform-administrator/security.html",
  "content": "# {{ page.title }}Learn how to safeguard your data with {{site.terms.sep_first}}&#39;s securitytoolkit in this training video presented by one of our founders, Dain Sundstrom.For your convenience, we&#39;ve divided the video training course up into topicsections, and provided links to the relevant parts of our documentation below.## Introduction             {% include youtubeSnippet.html id=page.youtubeId1 start=306 end=943 %}              Topics:              SEP security process        What to secure        Preparing: Verifying HTTP            Running time: ~11 min.      ## Client to server encryption             {% include youtubeSnippet.html id=page.youtubeId1 start=944 end=2096 %}              Topics:              Approaches for HTTPS, including proxies and load balancers        Adding SSL/TLS certificates        Handling PEM and JKS files        Verifying HTTPS for SEP            Running time: ~19 min.      ## Authentication and authorization in {{site.terms.sep}}             {% include youtubeSnippet.html id=page.youtubeId1 start=2097 end=4141 %}              Topics:              Password file authentication        LDAP authentication (See also: group providers)        Kerberos authentication (See also: passthrough)        Client certificate authentication        JSON Web Token authentication        Using multiple authenticators        Authentication with user mapping        Overview of authorization        File-based system access control            Running time: ~34 min.      ## Securing {{site.terms.sep}}&#39;s internal communications and management endpointsDocumentation for the material covered in this section is found[here]({{site.sep_url}}security/internal-communication.html).             {% include youtubeSnippet.html id=page.youtubeId1 start=4694 end=5608 %}              Topics:              Securing the Starburst cluster itself        Shared secret        Internal HTTPS        Secrets management        Management endpoints            Running time: ~16 min.      ## Hive catalog securityWe recommend the following additional reading, which covers enabling{{site.terms.sep}}&#39;s powerful role-based global access control:* [Access control overview]({{site.sep_url}}security/access-control.html)* [Global access control with Apache Ranger]({{site.sep_url}}security/global-ranger.html)* [Global access control with Privacera]({{site.sep_url}}security/global-privacera.html)* [Built-in system access control]({{site.sep_url}}security/built-in-system-access-control.html)While we strongly recommend implementing global access control, you can stillsecure Hive at the catalog level if your particular situation makes thatnecessary. Documentation covering the various options for securing Hive at thecatalog level can be found as follows:* [Configuring Hive security]({{site.sep_url}}connector/hive-security.html)* [Hive-level security with Apache Ranger]({{site.sep_url}}security/hive-ranger.html)* [Hive-level security with Privacera]({{site.sep_url}}security/hive-privacera.html)* [Hive-level security with Apache Sentry]({{site.sep_url}}security/hive-sentry.html)             {% include youtubeSnippet.html id=page.youtubeId1 start=5609 end=6650 %}              Topics:              Authorization        Metastore authentication        HDFS authentication        Kerberos debugging        S3 authentication        GCP authentication            Running time: ~18 min.      "
 },
 {
  "title": "Starburst user personas",
  "url": "/starburst-personas.html",
  "content": "# {{ page.title }}No matter what their job title is, most {{site.terms.sb}} users fit one of ouruser personas:* *Data consumers* query data through existing catalogs.* *Data engineers* create catalogs that connect data sources to Starburst.* *Platform administrators* run and maintain the Starburst cluster.These personas embody a set of focused workflows and assume a nominal skillset. Your role may comprise some or all of one or more of these personas&#39;workflows, and that is ok! In fact, we can easily describe this overlap:* All users in the course of their job are data consumers at some level.* Data engineers might do a bit of platform administration.* Platform administrators might do a bit of data engineering.Rather than repeat information or throw every possible workflow at every user,{{site.terms.sb}} user guides are organized to answer a very important question:&quot;Where do I start?!&quot; We use personas to accomplish this.Let&#39;s meet the {{site.terms.sb}} personas so that you can start with the oneclosest to your day-to-day workflows.## Data consumerData analysts, data scientists, and casual report wranglers all use{{site.terms.sb}} in very similar ways. In a nutshell, a data consumer focuseson one or more of the following:* Delivering visualizations and reports.* Making well-informed, data-driven decisions.* Creating forecast and machine learning models that describe the business.* Performing ad hoc analyses.In our guides, we assume a reasonable level of skill with[SQL](./glossary.html#sql), including some knowledge of more advanced queries,and some combination of the following:* Limited to reasonable programming skills* Excellent spreadsheet skills, including some modeling* Knowledge of statistical methods and/or machine learning* Competence with data visualization and/or reporting tools* Ability to detect and articulate issues with data, even if unable to  remedy them or trace the causeIt&#39;s worth noting that downstream users who only consume data through generatedreports and visualizations and don&#39;t actively query data themselves directly aredirect customers of data consumers. Our documentation does not teach basic SQLskills.Our [data consumer user guide]({{site.baseurl}}./data-consumer) provides muchinformation and in-depth training that covers:* Starburst clients* Query federation* Query optimization* Migrating queries to Starburst SQL## Data engineerData engineers deliver data to data consumers in a performant and suitableformat, and in a timely manner with an expectation of a given level of dataquality. Often they source new data from a variety of relational databases,object stores, log entries, message streams or product endpoints. In a nutshell,a data engineer focuses on:* Creating and updating data models &amp; data schemas.* Building and managing ETLs.* Identifying, designing, and implementing internal process improvements such as  automating manual processes, optimizing data delivery for greater scalability.* Building and managing streaming data pipelines.* Building data integrations between various 3rd party systems such as Salesforce  and Workday.Data engineers are customers to platform administrators and the servicesthey provide. Data consumers are the direct customers of data engineers.{{site.terms.sb}} lets data engineers [decouple compute fromstorage]({{site.baseurl}}./data-engineer) and simplifies delivering the data thatyour users need. Our [data engineer userguide]({{site.baseurl}}./starburst-enterprise/data-engineer) will get you started with detailedinformation and training on topics such as:* Creating catalogs to connect data sources.* Developing custom connectors.* Diagnosing and fixing query performance issues.## Platform administrator{{site.terms.sb}} platform administrators care about the scalability,performance and reliability of the {{site.terms.sb}} cluster. They balance SLAsfor both data landing times and availability; implement access and datagovernance policies; and support audit requirements. In a nutshell, platformadministrators focus on:* Building and maintaining scalable data platform architectures to support the  ingest, storage and querying of large heterogenous datasets.* Creating and monitoring cluster health metrics to ensure optimal performance  and reduce any downtime.* Implementing and working with cloud providers like AWS, Azure, and Google  Cloud.{{site.terms.sb}} runs on [COTS](./glossary.html#cots) hardware, and uses memoryinstead of disk, making it fast and more cost-effective. We&#39;ve put together muchin-depth information and training in our [platform administrator userguide](./starburst-enterprise/platform-administrator/index.html) for you,covering:* Security* Performance tuning* Cluster setup and configuration"
 },
 {
  "title": "Using SQL in Starburst",
  "url": "/data-consumer/starburst-sql.html",
  "content": "# {{ page.title }}{{site.terms.sb}} brings {{site.terms.oss}}&#39;s open source distributed SQL enginefor running fast analytic queries against various data sources ranging in sizefrom gigabytes to petabytes to even more data sources, with more robustfeatures. Because {{site.terms.sb}}&#39;s SQL is ANSI-compliant and supports most ofthe SQL language features you depend on, you can hit the ground running.Business intelligence users and data scientists can continue to use theirfavorite tools such as Tableau, Qlik and Apache Superset to access and analyzevirtually any data source, or multiple data sources in a single query.## The basicsWe know you want to jump right in, and we know you already have awesomeanalytics skills. It&#39;s just a matter of harnessing the power of{{site.terms.sb}} products to take your analytics even further. With that inmind, you can browse our latest reference materials to learn just how familiar{{site.terms.sb}} SQL is:* [SQL language]({{site.sep_url}}language.html)* [SQL statement syntax]({{site.sep_url}}sql.html)* [Functions and operators]({{site.sep_url}}functions.html)### CatalogsOne key difference worth highlighting is the concept of *catalogs*. Each of yourdata sources is defined as a catalog in {{site.terms.sb}}, and that catalog inturn contains schemas. Using a SQL client such as our CLI, you can discover whatcatalogs are available:```sqlpresto&gt; SHOW CATALOGS; Catalog--------- hive_sales mysql_crm(2 rows)```From there, you can use the familiar `SHOW SCHEMAS` command to drill furtherdown.### Fully-qualified table namesTable names are fully qualified when they include the catalog name:```..```This becomes critical when creating [federated queries](./query-federation.html).### General SQL featuresJust in case you&#39;d like a review, here&#39;s a walkthrough of some basic SQLfeatures in {{site.terms.sb}} from one of our founders, David Phillips:             {% include youtubeSnippet.html id=page.youtubeId1 start=585 end=1060 %}              Topics:              Formatting        CASE and searched CASE expressions        IF expressions        TRY expressions        Lambda expressions            Click on the links to read more on that topic in our reference manual.      Running time: ~8 min.      ## Advanced SQLReady to move past the basics? For your convenience, we&#39;ve divided the AdvancedSQL for {{site.terms.sb}} video training course up into topic sections, andprovided links to the relevant parts of our documentation below.### Advanced aggregation techniques             {% include youtubeSnippet.html id=page.youtubeId1 start=1970 end=3654 %}              Topics:              count() with DISTINCT        Approximations, including counting and percentiles        max_by() values        Pivoting with count_if() and FILTER        Complex aggregations        Checksums        ROLLUP        CUBE        GROUPING SETS            Running time: ~28 min.      ### Window functions             {% include youtubeSnippet.html id=page.youtubeId1 start=5280 end=6768 %}              Topics:              Row numbering        Ranking        Ranking and numbering without ordering        Bucketing and percentage ranking        Partitioning        Accessing leading and training rows with lead() and lag()         Window frames        Accessing first, last and Nth values        ROWS vs RANGE using array_agg()        Using aggregations in window functions            Running time: ~25 min.      ### Array and map functionsMany data stores allow to to create arrays, but it isn&#39;t always easy.{{site.terms.sb}} allows you to easily create arrays and maps with your data.Creating arrays with your data is easy:```sqlSELECT ARRAY[4, 5, 6] AS integers,       ARRAY[&#39;hello&#39;, &#39;world&#39;] AS varchars; integers  |   varchars-----------+---------------- [4, 5, 6] | [hello, world]```SQL array indexes are 1-based. Learn more about how to use and manipulate themin this in-depth video.             {% include youtubeSnippet.html id=page.youtubeId1 start=4067 end=5214 %}              Topics:              Accessing array and map elements with element_at()        Sorting arrays with array_sort()        matching elements with any_match(), all_match() and none_match()        Filtering elements        Transforming elements        Converting arrays to strings        Computing array products        Unnesting arrays and maps        Creating maps from keys and values and an array of entry rows            Running time: ~19 min.      ### Using JSON             {% include youtubeSnippet.html id=page.youtubeId1 start=1116 end=1969 %}              Topics:              The JSON data type        Extraction using json_extract() and json_extract_scalar()        Casting and partial casting from JSON        Formatting as JSON            Running time: ~14 min.      "
 },

 {
  "title": "Apache Superset",
  "url": "/data-consumer/clients/superset.html",
  "content": "# {{ page.title }}"
 },
 {
  "title": "Tableau",
  "url": "/data-consumer/clients/tableau.html",
  "content": "# {{ page.title }}You can use the popular analytics platform [Tableau](https://www.tableau.com/)with your {{site.terms.sb}} cluster.Using the JDBC driver is the preferred method, while the ODBC driver isfunctional as well.Read more about the specific details in [Tableau referencedocumentation]({{site.sep_url}}installation/tableau.html)."
 },
 {
  "title": "Metadata tags for page front matter",
  "url": "/internal/tag-list.html",
  "content": "# {{page.titles}}The tags in this document are adopted for use at this time. They are used asmetadata to collect and list pages.## searchindex tagIf set to `false`, the page is not included in the simple search, see`search.json`.## persona tagValues:* platform-administrator* data-engineer* data-consumer* data-leader## product tagValues:* sep* galaxy* marketplace* amazon-marketplace* azure-marketplace* gcp-marketplace* redhat-marketplace## media-type tagValues:* text* video* graphics* screenshot"
 },
 {
  "title": "Tags overview",
  "url": "/starburst-galaxy/metadata/tag-resources.html",
  "content": "# {{ page.title }}Tags are a label, or key-value pair, that you assign to organize and applymetadata to a [cluster](/starburst-galaxy/clusters.html).Create a tag by defining a key and assigning value to it. For example, define akey as a cost center and assign an associated region: `cost-center =boston`The {{site.terms.galaxy_first}} allows you to use tags from your AWS resources.Similar to AWS, you must manually assign a tag to a resource, it won&#39;tautomatically assign itself. Learn about [resource tagging on AWS](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html).## Add tags to {{site.terms.galaxy}}There are two places where you can add tags:1. The **Saved items** page2. In the **Configure additional parameters** section of the    [cluster creation process](/starburst-galaxy/clusters.html)   under the **Add tags** section#### Add tags from the Saved items pageAdd tags from the **Saved items** page.1. On the **Saved items** page, click **+ New**.3. In the **Key** field, type to add a new key or select an existing key.4. Assign a **value** to your key.5. Click **Save**.Navigate to **Saved items** to edit, delete, orview the tag usage at any time.#### Add a tag during clusters creationWhen you create a cluster, you have the option to add existing tags to thecluster or create a new tag.1. On the **Clusters** page, select **+ New**.2. Open **Configure additional parameters (optional)** and    select **Create a new tag**.3. In the **Key** field, type to add a new key or select an existing key.4. Assign a **value** to your key.5. Click **Save**.Navigate to **Saved items** to edit, delete, orview the tag usage at any time.## Edit a tag or view tag usageNavigate to **Saved items** to edit, delete, orview the tag usage at any time.#### Edit a tagAny updates you make to a tag apply to all of the clusters that use that tag.1. Click **Menu** next to the tag you want to update and select **Edit tag**.2. Change the **Key** or **Value**.3. Click **Save tag** to input your updates.Your tag is updated and will apply the new tag info to the clusters that use it.#### View tag usageSee which clusters are associated with a tag. 1. Click **Menu** next to the tag whose usage you want to view    and select **View usage**.2. The tag usage displays and shows how many items use the tag.While you&#39;re viewing a tag, you have the option to remove items that use it."
 },

 {
  "title": "Verify server with Web UI",
  "url": "/starburst-enterprise/try/verify.html",
  "content": "# {{page.title}}To verify that your locally run {{site.terms.sep_full}} server is running, usethe Web UI.With any modern browser, go to [http://localhost:8080](http://localhost:8080).At the login screen, enter your current OS login name (or any string).The default Web UI screen shows the version number, environment, and uptime ofthe server. The statistics fields show zeros until a query is run against theserver.{% include  modal.html  url=&#39;../../assets/img/general/web-ui-empty.png&#39;  img-id=&#39;webuiempty&#39;  alt-text=&#39;Empty Web UI screen&#39;  descr-text=&#39;Empty Web UI screen&#39;  pxwidth=&#39;440&#39;%}To run queries against the local server with the {{site.terms.oss}} CLI, see[CLI]({{site.baseurl}}/data-consumer/clients/cli)."
 },

 {
  "title": "",
  "url": "",
  "date": "",
  "content": ""
 }
]