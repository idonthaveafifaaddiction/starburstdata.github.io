==============================
Starburst Delta Lake Connector
==============================

The Delta Lake connector allows querying data stored in `Delta Lake
<https://delta.io>`_ format. It can natively read the Delta transaction log and
thus detect when external systems change data.

.. warning::
  This connector is currently available as beta release only. Please work with
  the Starburst support team, if you are planning to use this connector in
  production.

.. contents::
    :local:


Configuration
-------------

The connector requires a Hive metastore for table metadata and supports the same
metastore configuration properties as the :doc:`Hive connector
</connector/hive>`. At a minimum, ``hive.metastore.uri`` must be configured.

The connector recognizes Delta tables created in the metastore by the Databricks
runtime. If non-Delta tables are present in the metastore, as well, they will
not be visible to the connector.

To configure the Delta Lake connector, create a catalog file, for example
``etc/catalog/delta.properties``, that references the ``delta-lake`` connector.
Update the ``hive.metastore.uri`` with the URI of your Hive metastore Thrift
service:

.. code-block:: none

    connector.name=delta-lake
    hive.metastore.uri=thrift://example.net:9083

The Delta Lake connector reuses certain functionalities from the Hive connector.
To configure access to S3 or Azure storage, consult the Amazon S3 section of
:doc:`/connector/hive` or :doc:`/connector/sb-hive-azurestorage`,
respectively.

Authorization
-------------

The connector supports standard Hive security for authorization. For
configuration properties, see the 'Authorization' section in
:doc:`/connector/hive-security`.

Delta Lake-specific Configuration Properties
--------------------------------------------

.. list-table:: Configuration Properties
    :widths: 15, 60, 25
    :header-rows: 1

    * - Property name
      - Description
      - Default value
    * - ``delta.metadata.cache-ttl``
      - How frequently the connector checks for new Delta transactions.
      - 5 min
    * - ``delta.metadata.live-files.cache-size``
      - Amount of memory allocated for caching information about Delta files.
      - 10% of the maximum memory allocated to the master node JVM

Creating Tables
---------------

When Delta tables exist in storage, but not in the metastore, Presto can be used
to register them::

   CREATE TABLE delta.default.my_table (
      dummy bigint
    )
   WITH (
      external_location = '...'
   )

Note that the columns listed in the DDL, such as ``dummy`` in the above example,
are ignored. The table schema is read from the transaction log, instead. If the
schema is changed by an external system, Presto automatically uses the new
schema.

Statistics
----------

The Delta Lake specification defines a number of per file statistics that can be
included in the transaction log. When they are present, the connector uses them
to expose table and column level statistics, as documented in
:doc:`/optimizer/statistics`. Only the number of distinct values for a column is
not provided by the from the transaction log, and can therefore not be used for
:doc:`cost based optimizations </optimizer/cost-based-optimizations>`. This
results in less efficient optimization.

The Databricks runtime automatically collects and records statistics, while they
are missing for tables created by the open source Delta Lake implementation.

Limitations
-----------

Inserting, removing and updating data is not supported. The connector does not
support DDL statements, with the exception of ``CREATE TABLE``, as described
above, and ``CREATE SCHEMA``.
