Presto Coordinator High Availability
=====================================
Starburst Presto offers the ability to enable Presto Coordinator high availability (HA).
In the event the Coordinator becomes unavailable, this allows your Presto cluster to automatically switch over to a new Coordinator and continue to accept new queries.

Configuring Presto Coordinator HA
---------------------------------
Coordinator high availability (HA) is supported only via the Starburst CloudFormation template in AWS. This is where we tie together all the necessary components for this feature to work.
In order to fully utilize this capability set the ``HACoordinatorsCount`` field of the Stack creation form (``EC2 Configuration`` section) to a value greater than ``1``.
Setting it to ``2`` or ``3`` should suffice most scenarios.

HA is ALWAYS enabled. However in the when ``HACoordinatorsCount`` is set to ``1``, there is no hot standby.
In that case Starburst Presto will eventually create a new Coordinator and this may take several minutes.
If ``HACoordinatorsCount`` equals ``2`` or more, then there are hot standby Coordinators and the fail-over switch is faster.

Coordinator DNS Address
-----------------------
After you launch the Starburst CloudFormation cluster stack, note the ``PrestoCoordinatorURL`` and ``PrestoSSH`` keys in the stack's Outputs section/tab in the AWS CloudFormation console.
Both keys use a private AWS Hosted Zone setup by CloudFormation.

``PrestoCoordinatorURL`` is the Presto Web UI and REST API endpoint address, which you use to point your Presto CLI or JDBC/ODBC drivers or access the Presto Web UI from your browser.
``PrestoSSH`` notes the SSH connection details to manually log onto the current Presto Coordinator.

Fail Over Scenario(s)
---------------------

In general in the event of a failure of the current Presto Coordinator the HA mechanism will kick in and perform the following steps:

1. Terminate the old/failed Coordinator EC2 instance
2. Switch the private IP in the Coordinators DNS record to point to the new Coordinator
3. Launch a new stand-by coordinator (within a couple minutes)

The core fail-over process (steps 1 and 2) should complete in under a minute, from the time when the Coordinator started failing to respond.
It is a matter of seconds once the Coordinator is identified to be in a failed state, but there is some built-in time buffer so that we don't act on a false alarm.
The DNS record has a 5 TTL of 5 seconds, making it immediately visible to clients.

**In real life it may happen that a Coordinator "dies" because one of the following:**

* The node becomes unresponsive (eg. hardware issues, OS level and network issues).
* The node disappears, might be terminated by some account admin or by AWS.
* The Presto process may exit because of an fatal error.
* The Presto process may become unresponsive, e.g. because of a long full garbage collection taking place.

In all those scenarios, after a short grace period, the failed Coordinator, if still exists, is terminated.
Then a new Coordinator is chosen among the hot standby instances and it's private IP is written to the DNS record,
this should be immediately visible to clients, who should then re-issue the failed queries. A new hot standby coordinator is launched in the background to take place
of the one that has just been assigned.

Considerations
--------------
* Because the Coordinator address is defined via a AWS private Hosted Zone it is resolvable only within the same VPC as the Starburst Presto cluster stack.
  This means in order to connect to Presto you need to initiate the connection from a client either on EC2 machine deployed in the same VPC or connected to the VPC via a VPN.
* Please note that all queries that were running when the Coordinator failed, will also fail to complete. You will need to restart these queries on the new Coordinator.
  Similarly the SSH connections to the old Coordinator will need to be re-established after the fail-over.
* When connecting via SSH, depending on your SSH configuration you may see login issues like ``REMOTE HOST IDENTIFICATION HAS CHANGED`` etc, due to the fact that the underlying host
  has changed, and the key's fingerprint that was previously accepted has changed. You may want to not verify the host keys at all,
  by adding ``-o StrictHostKeyChecking=no`` to the SSH command or deleting the key from your ``known_hosts`` file and accepting the new one.
